{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/jianming/PONet/program/project/FactorVAE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "from accelerate import Accelerator"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "num_epochs = 25\n",
    "lr = 0.0001\n",
    "batch_size = 1024\n",
    "batch_gap = 1\n",
    "num_latent = 6 # feature size\n",
    "seq_len = 60\n",
    "num_factor = 32\n",
    "hidden_size = 32\n",
    "hidden_factor = 32\n",
    "seed = 42\n",
    "save_dir = r'/home/jianming/PONet/program/project/FactorVAE/data/best_models'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# train_df = pd.read_pickle(\"/home/jianming/PONet/program/project/FactorVAE/data/train_sp500_QLIB_False_NORM_False_CHAR_False_LEN_20.pkl\")\n",
    "# valid_df = pd.read_pickle(\"/home/jianming/PONet/program/project/FactorVAE/data/valid_sp500_QLIB_False_NORM_False_CHAR_False_LEN_20.pkl\")\n",
    "# test_df = pd.read_pickle(\"/home/jianming/PONet/program/project/FactorVAE/data/test_sp500_QLIB_False_NORM_False_CHAR_False_LEN_20.pkl\")\n",
    "# US_feature_dataset_market_sp500_valid_start2015-01-01_end2016-12-31_label5\n",
    "train_df = pd.read_pickle(\"/home/jianming/PONet/program/project/FactorVAE/data/US_feature_dataset_market_sp500_train_start2008-01-01_end2014-12-31_label20\")\n",
    "valid_df = pd.read_pickle(\"/home/jianming/PONet/program/project/FactorVAE/data/US_feature_dataset_market_sp500_valid_start2015-01-01_end2016-12-31_label20\")\n",
    "test_df = pd.read_pickle(\"/home/jianming/PONet/program/project/FactorVAE/data/US_feature_dataset_market_sp500_test_start2017-01-01_end2020-08-01_label20\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "data": {
      "text/plain": "                        CLOSE59   CLOSE58   CLOSE57   CLOSE56   CLOSE55  \\\ndatetime   instrument                                                     \n2010-01-04 A          -0.801470 -0.792347 -0.805499 -0.865277 -0.789545   \n           AA         -0.939941 -1.010894 -1.017612 -1.105897 -1.004793   \n           AAPL       -0.721489 -0.681588 -0.676562 -0.719535 -0.676018   \n           ABC        -1.130259 -1.073238 -1.041954 -1.094959 -0.954652   \n           ABT        -0.469372 -0.482687 -0.503474 -0.503206 -0.250961   \n...                         ...       ...       ...       ...       ...   \n2017-12-29 XYL        -0.202851 -0.197502 -0.190815 -0.199200 -0.248474   \n           YUM        -0.490197 -0.351213 -0.350005 -0.336936 -0.292932   \n           ZBH         0.177233  0.181779  0.049109  0.112167  0.126088   \n           ZION       -0.277664 -0.246040 -0.347252 -0.288158 -0.372653   \n           ZTS        -0.728868 -0.739231 -0.751547 -0.728709 -0.702852   \n\n                        CLOSE54   CLOSE53   CLOSE52   CLOSE51   CLOSE50  ...  \\\ndatetime   instrument                                                    ...   \n2010-01-04 A          -0.792142 -0.926031 -0.881090 -1.176907 -1.066803  ...   \n           AA         -0.995895 -1.188350 -1.186017 -1.361593 -1.361682  ...   \n           AAPL       -0.718102 -0.838528 -0.770304 -0.387089 -0.119082  ...   \n           ABC        -0.870498 -0.819805 -0.866395 -0.779866 -0.878006  ...   \n           ABT        -0.137176 -0.225172 -0.092749 -0.163453 -0.294764  ...   \n...                         ...       ...       ...       ...       ...  ...   \n2017-12-29 XYL        -0.178195 -0.207453 -0.041043 -0.426306 -0.371775  ...   \n           YUM        -0.326184 -0.357495 -0.363535 -0.374415 -0.402481  ...   \n           ZBH         0.274740  0.200915  0.183904  0.213350  0.271014  ...   \n           ZION       -0.482223 -0.512960 -0.486433 -0.645252 -0.622052  ...   \n           ZTS        -0.632751 -0.574249 -0.548104 -0.545166 -0.588158  ...   \n\n                        VOLUME9   VOLUME8   VOLUME7   VOLUME6   VOLUME5  \\\ndatetime   instrument                                                     \n2010-01-04 A           0.411700  0.298818 -0.209210 -0.420662 -1.603741   \n           AA          0.294136  2.791438  0.451646  0.003445 -0.998537   \n           AAPL        0.458316  0.476913 -0.632760 -0.662556  0.012183   \n           ABC         2.104521  0.139758 -0.390253  0.114148 -1.781350   \n           ABT         1.649119  0.253572 -0.155324 -1.024470 -1.570560   \n...                         ...       ...       ...       ...       ...   \n2017-12-29 XYL         1.498057  0.061790 -0.636312 -1.049967 -0.850350   \n           YUM         3.000000  0.647190 -0.203018 -0.321770  0.491947   \n           ZBH         2.270499  0.882518  3.000000  1.118766  0.122109   \n           ZION        3.000000  2.210243  0.105724  0.725600  0.580410   \n           ZTS         1.355472 -0.076837  0.280258  0.108470 -0.714964   \n\n                        VOLUME4   VOLUME3   VOLUME2   VOLUME1  VOLUME0  \ndatetime   instrument                                                   \n2010-01-04 A          -0.879167  0.303500  1.125745  1.020440      0.0  \n           AA         -0.611950 -1.061957 -0.739595 -1.068296      0.0  \n           AAPL        0.669912 -0.247133 -0.428418 -0.849569      0.0  \n           ABC        -1.233891 -0.973650 -0.862436 -1.182416      0.0  \n           ABT        -0.494855 -0.351178 -0.783187 -1.242245      0.0  \n...                         ...       ...       ...       ...      ...  \n2017-12-29 XYL        -1.539819 -1.857334 -1.269359 -1.614869      0.0  \n           YUM        -0.728033 -1.193908 -0.561972 -1.460446      0.0  \n           ZBH        -0.691201 -1.495628 -0.479991 -0.122525      0.0  \n           ZION       -0.695005 -0.854849 -1.130369 -0.678290      0.0  \n           ZTS        -0.487750 -1.263578 -0.808953 -1.696340      0.0  \n\n[865331 rows x 360 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>CLOSE59</th>\n      <th>CLOSE58</th>\n      <th>CLOSE57</th>\n      <th>CLOSE56</th>\n      <th>CLOSE55</th>\n      <th>CLOSE54</th>\n      <th>CLOSE53</th>\n      <th>CLOSE52</th>\n      <th>CLOSE51</th>\n      <th>CLOSE50</th>\n      <th>...</th>\n      <th>VOLUME9</th>\n      <th>VOLUME8</th>\n      <th>VOLUME7</th>\n      <th>VOLUME6</th>\n      <th>VOLUME5</th>\n      <th>VOLUME4</th>\n      <th>VOLUME3</th>\n      <th>VOLUME2</th>\n      <th>VOLUME1</th>\n      <th>VOLUME0</th>\n    </tr>\n    <tr>\n      <th>datetime</th>\n      <th>instrument</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">2010-01-04</th>\n      <th>A</th>\n      <td>-0.801470</td>\n      <td>-0.792347</td>\n      <td>-0.805499</td>\n      <td>-0.865277</td>\n      <td>-0.789545</td>\n      <td>-0.792142</td>\n      <td>-0.926031</td>\n      <td>-0.881090</td>\n      <td>-1.176907</td>\n      <td>-1.066803</td>\n      <td>...</td>\n      <td>0.411700</td>\n      <td>0.298818</td>\n      <td>-0.209210</td>\n      <td>-0.420662</td>\n      <td>-1.603741</td>\n      <td>-0.879167</td>\n      <td>0.303500</td>\n      <td>1.125745</td>\n      <td>1.020440</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>AA</th>\n      <td>-0.939941</td>\n      <td>-1.010894</td>\n      <td>-1.017612</td>\n      <td>-1.105897</td>\n      <td>-1.004793</td>\n      <td>-0.995895</td>\n      <td>-1.188350</td>\n      <td>-1.186017</td>\n      <td>-1.361593</td>\n      <td>-1.361682</td>\n      <td>...</td>\n      <td>0.294136</td>\n      <td>2.791438</td>\n      <td>0.451646</td>\n      <td>0.003445</td>\n      <td>-0.998537</td>\n      <td>-0.611950</td>\n      <td>-1.061957</td>\n      <td>-0.739595</td>\n      <td>-1.068296</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>AAPL</th>\n      <td>-0.721489</td>\n      <td>-0.681588</td>\n      <td>-0.676562</td>\n      <td>-0.719535</td>\n      <td>-0.676018</td>\n      <td>-0.718102</td>\n      <td>-0.838528</td>\n      <td>-0.770304</td>\n      <td>-0.387089</td>\n      <td>-0.119082</td>\n      <td>...</td>\n      <td>0.458316</td>\n      <td>0.476913</td>\n      <td>-0.632760</td>\n      <td>-0.662556</td>\n      <td>0.012183</td>\n      <td>0.669912</td>\n      <td>-0.247133</td>\n      <td>-0.428418</td>\n      <td>-0.849569</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>ABC</th>\n      <td>-1.130259</td>\n      <td>-1.073238</td>\n      <td>-1.041954</td>\n      <td>-1.094959</td>\n      <td>-0.954652</td>\n      <td>-0.870498</td>\n      <td>-0.819805</td>\n      <td>-0.866395</td>\n      <td>-0.779866</td>\n      <td>-0.878006</td>\n      <td>...</td>\n      <td>2.104521</td>\n      <td>0.139758</td>\n      <td>-0.390253</td>\n      <td>0.114148</td>\n      <td>-1.781350</td>\n      <td>-1.233891</td>\n      <td>-0.973650</td>\n      <td>-0.862436</td>\n      <td>-1.182416</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>ABT</th>\n      <td>-0.469372</td>\n      <td>-0.482687</td>\n      <td>-0.503474</td>\n      <td>-0.503206</td>\n      <td>-0.250961</td>\n      <td>-0.137176</td>\n      <td>-0.225172</td>\n      <td>-0.092749</td>\n      <td>-0.163453</td>\n      <td>-0.294764</td>\n      <td>...</td>\n      <td>1.649119</td>\n      <td>0.253572</td>\n      <td>-0.155324</td>\n      <td>-1.024470</td>\n      <td>-1.570560</td>\n      <td>-0.494855</td>\n      <td>-0.351178</td>\n      <td>-0.783187</td>\n      <td>-1.242245</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">2017-12-29</th>\n      <th>XYL</th>\n      <td>-0.202851</td>\n      <td>-0.197502</td>\n      <td>-0.190815</td>\n      <td>-0.199200</td>\n      <td>-0.248474</td>\n      <td>-0.178195</td>\n      <td>-0.207453</td>\n      <td>-0.041043</td>\n      <td>-0.426306</td>\n      <td>-0.371775</td>\n      <td>...</td>\n      <td>1.498057</td>\n      <td>0.061790</td>\n      <td>-0.636312</td>\n      <td>-1.049967</td>\n      <td>-0.850350</td>\n      <td>-1.539819</td>\n      <td>-1.857334</td>\n      <td>-1.269359</td>\n      <td>-1.614869</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>YUM</th>\n      <td>-0.490197</td>\n      <td>-0.351213</td>\n      <td>-0.350005</td>\n      <td>-0.336936</td>\n      <td>-0.292932</td>\n      <td>-0.326184</td>\n      <td>-0.357495</td>\n      <td>-0.363535</td>\n      <td>-0.374415</td>\n      <td>-0.402481</td>\n      <td>...</td>\n      <td>3.000000</td>\n      <td>0.647190</td>\n      <td>-0.203018</td>\n      <td>-0.321770</td>\n      <td>0.491947</td>\n      <td>-0.728033</td>\n      <td>-1.193908</td>\n      <td>-0.561972</td>\n      <td>-1.460446</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>ZBH</th>\n      <td>0.177233</td>\n      <td>0.181779</td>\n      <td>0.049109</td>\n      <td>0.112167</td>\n      <td>0.126088</td>\n      <td>0.274740</td>\n      <td>0.200915</td>\n      <td>0.183904</td>\n      <td>0.213350</td>\n      <td>0.271014</td>\n      <td>...</td>\n      <td>2.270499</td>\n      <td>0.882518</td>\n      <td>3.000000</td>\n      <td>1.118766</td>\n      <td>0.122109</td>\n      <td>-0.691201</td>\n      <td>-1.495628</td>\n      <td>-0.479991</td>\n      <td>-0.122525</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>ZION</th>\n      <td>-0.277664</td>\n      <td>-0.246040</td>\n      <td>-0.347252</td>\n      <td>-0.288158</td>\n      <td>-0.372653</td>\n      <td>-0.482223</td>\n      <td>-0.512960</td>\n      <td>-0.486433</td>\n      <td>-0.645252</td>\n      <td>-0.622052</td>\n      <td>...</td>\n      <td>3.000000</td>\n      <td>2.210243</td>\n      <td>0.105724</td>\n      <td>0.725600</td>\n      <td>0.580410</td>\n      <td>-0.695005</td>\n      <td>-0.854849</td>\n      <td>-1.130369</td>\n      <td>-0.678290</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>ZTS</th>\n      <td>-0.728868</td>\n      <td>-0.739231</td>\n      <td>-0.751547</td>\n      <td>-0.728709</td>\n      <td>-0.702852</td>\n      <td>-0.632751</td>\n      <td>-0.574249</td>\n      <td>-0.548104</td>\n      <td>-0.545166</td>\n      <td>-0.588158</td>\n      <td>...</td>\n      <td>1.355472</td>\n      <td>-0.076837</td>\n      <td>0.280258</td>\n      <td>0.108470</td>\n      <td>-0.714964</td>\n      <td>-0.487750</td>\n      <td>-1.263578</td>\n      <td>-0.808953</td>\n      <td>-1.696340</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>865331 rows × 360 columns</p>\n</div>"
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['feature']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(\n",
    "        self,\n",
    "        df_feature,\n",
    "        df_label,\n",
    "        d_feat = 6,\n",
    "        batch_size=1024,\n",
    "        batch_gap = 20,\n",
    "        shuffle=False,\n",
    "        device=None,\n",
    "    ):\n",
    "        assert len(df_feature) == len(df_label)\n",
    "        self.device = device\n",
    "\n",
    "        self.df_feature = df_feature\n",
    "        self.df_label = df_label\n",
    "\n",
    "        self.batch_gap = batch_gap\n",
    "        self.feature = torch.from_numpy(self.df_feature.values)\n",
    "        self.label = torch.from_numpy(self.df_label.values)\n",
    "\n",
    "        self.d_feat = d_feat\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label) // (self.batch_size*self.batch_gap)\n",
    "\n",
    "    def iter_batch(self):\n",
    "        indices = np.arange(len(self.label))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(indices)\n",
    "\n",
    "        for i in range(len(indices))[:: (self.batch_size*self.batch_gap)]:\n",
    "            if len(indices) - i < self.batch_size:\n",
    "                break\n",
    "            yield i, indices[i : i + self.batch_size]\n",
    "\n",
    "    def get_batch(self, i, slc):\n",
    "        outs = self.feature[slc].view(len(slc),self.d_feat, -1), self.label[slc]# No date in iter batch\n",
    "        return outs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_df['feature'].fillna(0),train_df['label'].fillna(0), batch_gap = 20)\n",
    "valid_dataloader = DataLoader(valid_df['feature'].fillna(0),valid_df['label'].fillna(0), batch_gap = 1)\n",
    "test_dataloader = DataLoader(test_df['feature'].fillna(0),test_df['label'].fillna(0), batch_gap = 1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "42"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, num_latent, hidden_size, num_layers=1):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        self.num_latent = num_latent\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.normalize = nn.LayerNorm(num_latent)\n",
    "        self.linear = nn.Linear(num_latent, num_latent)\n",
    "        self.leakyrelu = nn.LeakyReLU()\n",
    "        self.gru = nn.GRU(num_latent, hidden_size, num_layers, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #! x: (batch_size, seq_length, num_latent)\n",
    "        # Apply linear and LeakyReLU activation\n",
    "        #* layer norm\n",
    "        if x.shape[-1] != self.num_latent:\n",
    "            x = x.permute((0,2,1))\n",
    "        x = self.normalize(x)\n",
    "        out = self.linear(x)\n",
    "        out = self.leakyrelu(out)\n",
    "        # Forward propagate GRU\n",
    "        stock_latent, _ = self.gru(out)\n",
    "        return stock_latent[:,-1,:] #* stock_latent[-1]: (batch_size, hidden_size)\n",
    "\n",
    "class FactorEncoder(nn.Module):\n",
    "    def __init__(self, num_factors, num_portfolio, hidden_size):\n",
    "        super(FactorEncoder, self).__init__()\n",
    "        self.num_factors = num_factors\n",
    "        self.linear = nn.Linear(hidden_size, num_portfolio)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "        self.linear2 = nn.Linear(num_portfolio, num_factors)\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "    def mapping_layer(self, portfolio_return):\n",
    "        #! portfolio_return: (batch_size, 1)\n",
    "        #! mapping layer\n",
    "        # print(portfolio_return.shape)\n",
    "        mean = self.linear2(portfolio_return.squeeze(1))\n",
    "        sigma = self.softplus(mean)\n",
    "        return mean, sigma\n",
    "\n",
    "    def forward(self, stock_latent, returns):\n",
    "        #! stock_latent: (batch_size, hidden_size)\n",
    "        #! returns: (batch_size, 1)\n",
    "        #! make portfolio\n",
    "        weights = self.linear(stock_latent)\n",
    "        weights = self.softmax(weights) # (batch_size, num_portfolio)\n",
    "\n",
    "        # multiply weights and returns\n",
    "        #print(f\"weights shape: {weights.shape}, returns shape: {returns.shape}\") # [300, 20], [300, 1]\n",
    "        # check returns.shape is tuple\n",
    "        if returns.dim() == 1:\n",
    "            returns = returns.unsqueeze(1)\n",
    "        portfolio_return = torch.mm(weights.transpose(1,0), returns) #* portfolio_return: (M, 1)\n",
    "        #print(f\"portfolio_return shape: {portfolio_return.shape}\")\n",
    "\n",
    "        return self.mapping_layer(portfolio_return)\n",
    "\n",
    "class AlphaLayer(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(AlphaLayer, self).__init__()\n",
    "        self.linear1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.leakyrelu = nn.LeakyReLU()\n",
    "        self.mu_layer = nn.Linear(hidden_size, 1)\n",
    "        self.sigma_layer = nn.Linear(hidden_size, 1)\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "    def forward(self, stock_latent):\n",
    "        #* stock latent는 FeatureExtractor에서 나온 것 (batch_size, hidden_size)\n",
    "        stock_latent = self.linear1(stock_latent)\n",
    "        stock_latent = self.leakyrelu(stock_latent)\n",
    "        alpha_mu = self.mu_layer(stock_latent)\n",
    "        alpha_sigma = self.sigma_layer(stock_latent)\n",
    "        return alpha_mu, self.softplus(alpha_sigma)\n",
    "\n",
    "class BetaLayer(nn.Module):\n",
    "    \"\"\"calcuate factor exposure beta(N*K)\"\"\"\n",
    "    def __init__(self, hidden_size, num_factors):\n",
    "        super(BetaLayer, self).__init__()\n",
    "        self.linear1 = nn.Linear(hidden_size, num_factors)\n",
    "\n",
    "    def forward(self, stock_latent):\n",
    "        beta = self.linear1(stock_latent)\n",
    "        return beta\n",
    "\n",
    "class FactorDecoder(nn.Module):\n",
    "    def __init__(self, alpha_layer, beta_layer):\n",
    "        super(FactorDecoder, self).__init__()\n",
    "\n",
    "        self.alpha_layer = alpha_layer\n",
    "        self.beta_layer = beta_layer\n",
    "\n",
    "    def reparameterize(self, mu, sigma):\n",
    "        eps = torch.randn_like(sigma)\n",
    "        return mu + eps * sigma\n",
    "\n",
    "    def return_mu(self,stock_latent, factor_mu, factor_sigma):\n",
    "        alpha_mu, alpha_sigma = self.alpha_layer(stock_latent)\n",
    "        #print(f\"alpha_mu shape: {alpha_mu.shape}, alpha_sigma shape: {alpha_sigma.shape}\")\n",
    "        beta = self.beta_layer(stock_latent)\n",
    "\n",
    "        factor_mu = factor_mu.view(-1, 1)\n",
    "        factor_sigma = factor_sigma.view(-1, 1)\n",
    "\n",
    "        # Replace any zero values in factor_sigma with a small value\n",
    "        factor_sigma[factor_sigma == 0] = 1e-6\n",
    "        #print(f\"factor_mu shape: {factor_mu.shape}, factor_sigma shape: {factor_sigma.shape}\")\n",
    "        #print(f\"beta shape: {beta.shape}\")\n",
    "        mu = alpha_mu + torch.matmul(beta, factor_mu)\n",
    "        return mu\n",
    "\n",
    "    def forward(self, stock_latent, factor_mu, factor_sigma):\n",
    "        #! warning: alpha_mu, alpha_sigma -> (N), (N)\n",
    "        alpha_mu, alpha_sigma = self.alpha_layer(stock_latent)\n",
    "        #print(f\"alpha_mu shape: {alpha_mu.shape}, alpha_sigma shape: {alpha_sigma.shape}\")\n",
    "        beta = self.beta_layer(stock_latent)\n",
    "\n",
    "        factor_mu = factor_mu.view(-1, 1)\n",
    "        factor_sigma = factor_sigma.view(-1, 1)\n",
    "\n",
    "        # Replace any zero values in factor_sigma with a small value\n",
    "        factor_sigma[factor_sigma == 0] = 1e-6\n",
    "        #print(f\"factor_mu shape: {factor_mu.shape}, factor_sigma shape: {factor_sigma.shape}\")\n",
    "        #print(f\"beta shape: {beta.shape}\")\n",
    "        mu = alpha_mu + torch.matmul(beta, factor_mu)\n",
    "        sigma = torch.sqrt(alpha_sigma**2 + torch.matmul(beta**2, factor_sigma**2) + 1e-6)\n",
    "        return self.reparameterize(mu, sigma)\n",
    "\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "\n",
    "        self.query = nn.Parameter(torch.randn(hidden_size))\n",
    "        self.key_layer = nn.Linear(hidden_size, hidden_size)\n",
    "        self.value_layer = nn.Linear(hidden_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, stock_latent):\n",
    "        #* calculate attention weights\n",
    "\n",
    "        self.key = self.key_layer(stock_latent)\n",
    "        self.value = self.value_layer(stock_latent)\n",
    "\n",
    "        attention_weights = torch.matmul(self.query, self.key.transpose(1,0)) # (N)\n",
    "        #* scaling\n",
    "        attention_weights = attention_weights / torch.sqrt(torch.tensor(self.key.shape[0])+ 1e-6)\n",
    "        # print(f\"attention_weights shape: {attention_weights.shape}\")\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        attention_weights = F.relu(attention_weights) # max(0, x)\n",
    "        attention_weights = F.softmax(attention_weights, dim=0) # (N)\n",
    "\n",
    "        #! calculate context vector\n",
    "        if torch.isnan(attention_weights).any() or torch.isinf(attention_weights).any():\n",
    "            return torch.zeros_like(self.value[0])\n",
    "        else:\n",
    "            context_vector = torch.matmul(attention_weights, self.value) # (H)\n",
    "            return context_vector\n",
    "\n",
    "class FactorPredictor(nn.Module):\n",
    "    def __init__(self, batch_size, hidden_size, num_factor):\n",
    "        super(FactorPredictor, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        self.num_factor = num_factor\n",
    "        self.attention_layers = nn.ModuleList([AttentionLayer(self.hidden_size) for _ in range(num_factor)])\n",
    "\n",
    "        self.linear = nn.Linear(hidden_size, hidden_size)\n",
    "        self.leakyrelu = nn.LeakyReLU()\n",
    "        self.mu_layer = nn.Linear(hidden_size, 1)\n",
    "        self.sigma_layer = nn.Linear(hidden_size, 1)\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "    def forward(self, stock_latent):\n",
    "        for i in range(self.num_factor):\n",
    "            attention_layer = self.attention_layers[i](stock_latent)\n",
    "            if i == 0:\n",
    "                h_multi = attention_layer\n",
    "            else:\n",
    "                h_multi = torch.cat((h_multi, attention_layer), dim=0)\n",
    "        h_multi = h_multi.view(self.num_factor, -1)\n",
    "\n",
    "        # print(\"h_multi:\", h_multi.shape)\n",
    "        h_multi = self.linear(h_multi)\n",
    "        h_multi = self.leakyrelu(h_multi)\n",
    "        pred_mu = self.mu_layer(h_multi)\n",
    "        pred_sigma = self.sigma_layer(h_multi)\n",
    "        pred_sigma = self.softplus(pred_sigma)\n",
    "        pred_mu = pred_mu.view(-1)\n",
    "        pred_sigma = pred_sigma.view(-1)\n",
    "        return pred_mu, pred_sigma\n",
    "\n",
    "class FactorVAE(nn.Module):\n",
    "    def __init__(self, feature_extractor, factor_encoder, factor_decoder, factor_predictor, risk_extrator, hidden_factor):\n",
    "        super(FactorVAE, self).__init__()\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.factor_encoder = factor_encoder\n",
    "        self.factor_decoder = factor_decoder\n",
    "        self.factor_predictor = factor_predictor\n",
    "        self.risk_extrator = risk_extrator\n",
    "        self.hidden_factor = hidden_factor\n",
    "        self.map = nn.Linear(64,hidden_factor)\n",
    "\n",
    "    @staticmethod\n",
    "    def KL_Divergence(mu1, sigma1, mu2, sigma2):\n",
    "        #! mu1, mu2: (batch_size, 1)\n",
    "        #! sigma1, sigma2: (batch_size, 1)\n",
    "        #! output: (batch_size, 1)\n",
    "        kl_div = (torch.log(sigma2/ sigma1) + (sigma1**2 + (mu1 - mu2)**2) / (2 * sigma2**2) - 0.5).sum()\n",
    "        return kl_div\n",
    "\n",
    "    def forward(self, x, returns):\n",
    "        #! x: (batch_size, seq_length, num_latent)\n",
    "        #! returns: (batch_size, 1)\n",
    "\n",
    "        stock_latent = self.feature_extractor(x)\n",
    "        risk_latent = self.risk_extrator(x)\n",
    "        stock_latent = torch.cat([stock_latent,risk_latent],dim=1)\n",
    "        stock_latent = self.map(stock_latent)\n",
    "        # corr = torch.corrcoef(stock_latent.transpose(1,0))\n",
    "        # U,_,_ = torch.pca_lowrank(corr, q=self.hidden_factor, center=True, niter=2)\n",
    "        # stock_latent = torch.mm(stock_latent,U)/torch.sqrt(torch.Tensor(len(stock_latent)))\n",
    "\n",
    "        factor_mu, factor_sigma = self.factor_encoder(stock_latent, returns)\n",
    "        reconstruction = self.factor_decoder(stock_latent, factor_mu, factor_sigma)\n",
    "        pred_mu, pred_sigma = self.factor_predictor(stock_latent)\n",
    "\n",
    "        # print(f\"pred_mu: {pred_mu.shape}, pred_sigma: {pred_sigma.shape}\")\n",
    "        # Define VAE loss function with reconstruction loss and KL divergence\n",
    "        #* Some adjustment\n",
    "        #* stock_adj: number of stocks that have no return data\n",
    "        stock_adj = 0\n",
    "        for i in range(len(returns)-1,-1,-1):\n",
    "            if returns[i] == 0:\n",
    "                stock_adj += 1\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        if stock_adj > 0:\n",
    "            reconstruction_loss = F.mse_loss(reconstruction[:-stock_adj], returns[:-stock_adj])\n",
    "        else:\n",
    "            reconstruction_loss = F.mse_loss(reconstruction, returns)\n",
    "\n",
    "        # Calculate KL divergence between two Gaussian distributions\n",
    "        if torch.any(pred_sigma == 0):\n",
    "            pred_sigma[pred_sigma == 0] = 1e-6\n",
    "        kl_divergence = self.KL_Divergence(factor_mu, factor_sigma, pred_mu, pred_sigma)\n",
    "        # regularization_loss = 0\n",
    "        # prediction_loss = F.mse_loss(self.prediction(x),returns)\n",
    "        vae_loss = reconstruction_loss + kl_divergence\n",
    "        # print(\"loss: \", vae_loss)\n",
    "        return vae_loss, reconstruction, factor_mu, factor_sigma, pred_mu, pred_sigma #! reconstruction, factor_mu, factor_sigma\n",
    "\n",
    "    # 학습 이후 사용\n",
    "    def prediction(self, x):\n",
    "        stock_latent = self.feature_extractor(x)\n",
    "        pred_mu, pred_sigma = self.factor_predictor(stock_latent)\n",
    "        y_pred = self.factor_decoder.return_mu(stock_latent, pred_mu, pred_sigma)\n",
    "        return y_pred\n",
    "\n",
    "    def latent_factor(self, x):\n",
    "        stock_latent = self.feature_extractor(x)\n",
    "        pred_mu, pred_sigma = self.factor_predictor(stock_latent)\n",
    "        return (pred_mu, pred_sigma)\n",
    "\n",
    "class GATModel(nn.Module):\n",
    "    def __init__(self, d_feat=158, hidden_size=64, num_layers=2, dropout=0.0, base_model=\"GRU\"):\n",
    "        super().__init__()\n",
    "\n",
    "        if base_model == \"GRU\":\n",
    "            self.rnn = nn.GRU(\n",
    "                input_size=d_feat,\n",
    "                hidden_size=hidden_size,\n",
    "                num_layers=num_layers,\n",
    "                batch_first=True,\n",
    "                dropout=dropout,\n",
    "            )\n",
    "        elif base_model == \"LSTM\":\n",
    "            self.rnn = nn.LSTM(\n",
    "                input_size=d_feat,\n",
    "                hidden_size=hidden_size,\n",
    "                num_layers=num_layers,\n",
    "                batch_first=True,\n",
    "                dropout=dropout,\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"unknown base model name `%s`\" % base_model)\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.d_feat = d_feat\n",
    "        self.transformation = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.a = nn.Parameter(torch.randn(self.hidden_size * 2, 1))\n",
    "        self.a.requires_grad = True\n",
    "        self.fc = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.fc_out = nn.Linear(hidden_size, hidden_size)\n",
    "        self.leaky_relu = nn.LeakyReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def cal_attention(self, x, y):\n",
    "        x = self.transformation(x)\n",
    "        y = self.transformation(y)\n",
    "\n",
    "        sample_num = x.shape[0]\n",
    "        dim = x.shape[1]\n",
    "        e_x = x.expand(sample_num, sample_num, dim)\n",
    "        e_y = torch.transpose(e_x, 0, 1)\n",
    "        attention_in = torch.cat((e_x, e_y), 2).view(-1, dim * 2)\n",
    "        self.a_t = torch.t(self.a)\n",
    "        attention_out = self.a_t.mm(torch.t(attention_in)).view(sample_num, sample_num)\n",
    "        attention_out = self.leaky_relu(attention_out)\n",
    "        att_weight = self.softmax(attention_out)\n",
    "        return att_weight\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [N, F*T]\n",
    "        x = x.reshape(len(x), self.d_feat, -1)  # [N, F, T]\n",
    "        x = x.permute(0, 2, 1)  # [N, T, F]\n",
    "        out, _ = self.rnn(x)\n",
    "        hidden = out[:, -1, :]\n",
    "        att_weight = self.cal_attention(hidden, hidden)\n",
    "        hidden = att_weight.mm(hidden) + hidden\n",
    "        hidden = self.fc(hidden)\n",
    "        hidden = self.leaky_relu(hidden)\n",
    "        return self.fc_out(hidden).squeeze() # [N, hidden size]\n",
    "\n",
    "class RiskFeatureExtractor(nn.Module):\n",
    "    \"\"\"supervise variance-covariance matrix reconstuction\"\"\"\n",
    "    def __init__(self, num_latent, hidden_size,num_layers=2,dropout=0.2,base_model = 'GRU'):\n",
    "        super(RiskFeatureExtractor, self).__init__()\n",
    "        self.num_latent = num_latent\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.normalize = nn.LayerNorm(self.num_latent)\n",
    "        self.linear = nn.Linear(self.num_latent, self.num_latent)\n",
    "        self.leakyrelu = nn.LeakyReLU()\n",
    "        self.GAT_model = GATModel(\n",
    "            d_feat=self.num_latent,\n",
    "            hidden_size=self.hidden_size,\n",
    "            num_layers=num_layers ,\n",
    "            dropout=dropout,\n",
    "            base_model=base_model,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.shape[-1] != self.num_latent:\n",
    "            x = x.permute((0,2,1))\n",
    "        x = self.normalize(x)\n",
    "        out = self.linear(x)\n",
    "        out = self.leakyrelu(out)\n",
    "        risk_latent = self.GAT_model(out)\n",
    "        return risk_latent #* stock_latent[-1]: (batch_size, hidden_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "set_seed(seed)\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "# create model\n",
    "feature_extractor = FeatureExtractor(num_latent=num_latent, hidden_size=hidden_size)\n",
    "risk_extrator = RiskFeatureExtractor(num_latent=num_latent, hidden_size = hidden_size)\n",
    "factor_encoder = FactorEncoder(num_factors=num_factor, num_portfolio=num_factor, hidden_size=hidden_factor)\n",
    "alpha_layer = AlphaLayer(hidden_factor)\n",
    "beta_layer = BetaLayer(hidden_factor, num_factor)\n",
    "factor_decoder = FactorDecoder(alpha_layer, beta_layer)\n",
    "factor_predictor = FactorPredictor(batch_size, hidden_factor, num_factor)\n",
    "factorVAE = FactorVAE(feature_extractor, factor_encoder, factor_decoder, factor_predictor, risk_extrator, hidden_factor)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def train(factor_model, dataloader, optimizer, batch_size, masked = False, benchmark = False):\n",
    "    factor_model.to(device)\n",
    "    factor_model.train()\n",
    "    total_loss = 0\n",
    "    reconstruction_loss = 0\n",
    "    kl_loss = 0\n",
    "    risk_loss = 0\n",
    "    ic = 0\n",
    "    mask_id = torch.randint(0, 500, (30,))\n",
    "    pbar = tqdm(range(len(dataloader)), desc=\"Training\", position=0,dynamic_ncols = True)\n",
    "    for (i, slc),_ in zip(dataloader.iter_batch(),pbar):\n",
    "        char, returns = dataloader.get_batch(i, slc)\n",
    "        if char.size(0)!=batch_size:\n",
    "            continue\n",
    "        inputs = char\n",
    "        labels = returns\n",
    "        inputs = inputs.float().to(device)\n",
    "        labels = labels.float().to(device)\n",
    "\n",
    "        if masked:\n",
    "            inputs[mask_id,:,:] = 0\n",
    "\n",
    "        if torch.isnan(inputs).any() or torch.isnan(labels).any():\n",
    "            continue\n",
    "        optimizer.zero_grad()\n",
    "        if benchmark:\n",
    "            pred = factor_model(inputs.view(batch_size,-1))\n",
    "            reconstruction, factor_mu, factor_sigma, pred_mu, pred_sigma = torch.nan,torch.nan,torch.nan,torch.nan, torch.nan\n",
    "            loss = F.mse_loss(pred.unsqueeze(1),labels)\n",
    "        else:\n",
    "            loss, reconstruction, factor_mu, factor_sigma, pred_mu, pred_sigma = factor_model(inputs, labels)\n",
    "            pred = factor_model.prediction(inputs)\n",
    "        if loss>10:\n",
    "            continue\n",
    "        total_loss += loss.item() * inputs.size(0)\n",
    "        # loss.backward()\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "        nn.utils.clip_grad_norm_(factor_model.parameters(), 1.0)\n",
    "        try:\n",
    "            reconstruction_loss += reconstruction.squeeze().mean().item()\n",
    "            kl_loss += loss.item() + reconstruction.squeeze().mean().item()\n",
    "        except:\n",
    "            reconstruction_loss = 0\n",
    "            kl_loss = 0\n",
    "        # risk_loss += risk.squeeze().mean().item()\n",
    "        # pred = factor_model.prediction(inputs)\n",
    "        ic += np.corrcoef(pred.squeeze().detach().cpu().numpy().T,labels.squeeze().detach().cpu().numpy().T)[1,0]\n",
    "        pbar.set_postfix({\"Train Loss\": loss.item()})\n",
    "\n",
    "        # print(loss)\n",
    "    ic = ic / len(dataloader)\n",
    "    print('ic',ic)\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    reconstruction_loss = reconstruction_loss/len(dataloader)\n",
    "    kl_loss = kl_loss / len(dataloader)\n",
    "    return avg_loss, reconstruction_loss, kl_loss, risk_loss\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(factor_model, dataloader, batch_size, benchmark = False):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    factor_model.to(device)\n",
    "    factor_model.eval()\n",
    "    total_loss = 0\n",
    "    reconstruction_loss = 0\n",
    "    ic = 0\n",
    "    kl_loss = 0\n",
    "    risk_loss = 0\n",
    "    for i, slc in dataloader.iter_batch():\n",
    "        char, returns = dataloader.get_batch(i, slc)\n",
    "        if char.size(0)!=batch_size:\n",
    "            continue\n",
    "        inputs = char\n",
    "        labels = returns[:,-1].reshape(-1,1)\n",
    "        inputs = inputs.float().to(device)\n",
    "        labels = labels.float().to(device)\n",
    "        if benchmark:\n",
    "            pred = factor_model(inputs.view(batch_size,-1))\n",
    "            reconstruction, factor_mu, factor_sigma, pred_mu, pred_sigma = torch.nan,torch.nan,torch.nan,torch.nan, torch.nan\n",
    "            loss = F.mse_loss(pred.unsqueeze(1),labels)\n",
    "        else:\n",
    "            loss, reconstruction, factor_mu, factor_sigma, pred_mu, pred_sigma = factor_model(inputs, labels)\n",
    "            pred = factor_model.prediction(inputs)\n",
    "\n",
    "        total_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        try:\n",
    "            reconstruction_loss += reconstruction.squeeze().mean().item()\n",
    "            kl_loss += loss.item() + reconstruction.squeeze().mean().item()\n",
    "        except:\n",
    "            reconstruction_loss = 0\n",
    "            kl_loss = 0\n",
    "\n",
    "        # reconstruction_loss += reconstruction.squeeze().mean().item()\n",
    "        # # risk_loss += risk.squeeze().mean().item()\n",
    "        # kl_loss += loss.item() + reconstruction.squeeze().mean().item()\n",
    "        ic += np.corrcoef(pred.squeeze().detach().cpu().numpy().T,labels.squeeze().detach().cpu().numpy().T)[1,0]\n",
    "    ic = ic / len(dataloader)\n",
    "    print('Test ic',ic)\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    reconstruction_loss = reconstruction_loss/len(dataloader)\n",
    "    kl_loss = kl_loss / len(dataloader)\n",
    "    return avg_loss, reconstruction_loss, kl_loss, risk_loss\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(factor_model, dataloader, seq_len):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    factor_model.to(device)\n",
    "    factor_model.eval()\n",
    "    total_loss = 0\n",
    "    with tqdm(total=len(dataloader)-seq_len+1) as pbar:\n",
    "        for char, returns,idx in dataloader:\n",
    "            if char.shape[1] != seq_len:\n",
    "                continue\n",
    "            inputs = char.to(device)\n",
    "            labels = returns.to(device)\n",
    "            inputs = inputs.float()\n",
    "            labels = labels.float()\n",
    "\n",
    "            loss, reconstruction, factor_mu, factor_sigma, pred_mu, pred_sigma = factor_model(inputs, labels)\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "            pbar.update(1)\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    return avg_loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, d_feat, hidden_size=128, num_layers=3, dropout=0.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mlp = nn.Sequential()\n",
    "        for i in range(num_layers):\n",
    "            if i > 0:\n",
    "                self.mlp.add_module(\"drop_%d\" % i, nn.Dropout(dropout))\n",
    "            self.mlp.add_module(\"fc_%d\" % i, nn.Linear(d_feat if i == 0 else hidden_size, hidden_size))\n",
    "            self.mlp.add_module(\"bd_%d\" % i, nn.BatchNorm1d(hidden_size))\n",
    "            self.mlp.add_module(\"relu_%d\" % i, nn.ReLU())\n",
    "\n",
    "        self.mlp.add_module(\"fc_out\", nn.Linear(hidden_size, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x).squeeze()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "model = MLP(d_feat = 360).cuda()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "optimizer = torch.optim.Adam(factorVAE.parameters(), lr=0.001)\n",
    "accelerator = Accelerator()\n",
    "# model_name = '/home/jianming/PONet/program/project/FactorVAE/data/best_models/sp500model_with_riskmodel_period20_64.pt'\n",
    "# factorVAE.load_state_dict(torch.load(model_name))\n",
    "train_dataloader, factorVAE, optimizer = accelerator.prepare(train_dataloader, factorVAE, optimizer)\n",
    "# optimizer = torch.optim.SGD(factorVAE.parameters(), lr=0.0001, momentum=0.9)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "Training:   0%|                                                                                               …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d409361428ac4cc2a6636011853313e6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Training:   0%|                                                                                               …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "211e0f88e3384014b59e7b6cafe2ad42"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ic 0.037219409354693936\n",
      "Test ic 0.022824588839403897\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training:   0%|                                                                                               …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b04afc0677e0455487cf3380f95507f1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ic 0.025201832977490637\n",
      "Test ic 0.022763590171739158\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training:   0%|                                                                                               …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0300fba5375a47019d81349003f4d56e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ic 0.022317068125869404\n",
      "Test ic 0.023146977733501564\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training:   0%|                                                                                               …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "70f5330ac02a41678e02ee2d27cc2b74"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ic 0.024223523723187985\n",
      "Test ic 0.024257286099280104\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training:   0%|                                                                                               …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d94e3bb0eb924b70a303fdc4f4b74c26"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ic 0.02610446303806612\n",
      "Test ic 0.02489320112092215\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training:   0%|                                                                                               …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c9fd9c6fff244988b31a1fb003ff8932"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ic 0.026666621514207015\n",
      "Test ic 0.024794412789843435\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training:   0%|                                                                                               …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "076e35730aad49538dbe39eefd9fa1e1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ic 0.026918119574116214\n",
      "Test ic 0.025384514170439125\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training:   0%|                                                                                               …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "afc73a060b4d4768944e6c26da44c14b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ic 0.02557126515113851\n",
      "Test ic 0.024697939552424986\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training:   0%|                                                                                               …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3303ea9b95da4cedb05a5d2b586d3318"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ic 0.02559985503030583\n",
      "Test ic 0.026025485483034374\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training:   0%|                                                                                               …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "102dcdb2a7a745de9ca9b9c76f522ccf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ic 0.025159696690139136\n",
      "Test ic 0.025507740608200883\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training:   0%|                                                                                               …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c17aac16f2b1405b969921d3900bcf5c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ic 0.025293564919263496\n",
      "Test ic 0.025997108465959297\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training:   0%|                                                                                               …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0e0a38c31194405ab745716d467bc6c5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ic 0.02517946365710858\n",
      "Test ic 0.025701172891001208\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training:   0%|                                                                                               …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cdb96a34d7174697b8cc56184ad53c72"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ic 0.024410304412239132\n",
      "Test ic 0.024853493479558827\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training:   0%|                                                                                               …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "eec7e10b80ee498d89418574eb49d999"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ic 0.022602604908651566\n",
      "Test ic 0.026002666771202144\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training:   0%|                                                                                               …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ffe96f08d86f4c558df0461eb3198a32"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ic 0.02373691905466734\n",
      "Test ic 0.025563560152952178\n"
     ]
    }
   ],
   "source": [
    "run_name = 'sp500model_with_riskmodel_periodm2002pic'\n",
    "num_epochs = 15\n",
    "train_loss_lst = []\n",
    "valid_loss_lst = []\n",
    "pbar = tqdm(range(num_epochs), desc=\"Training\", position=0,dynamic_ncols = True)\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "for e in pbar:\n",
    "    train_loss, train_conloss, train_kl, train_risk = train(factorVAE, train_dataloader, optimizer, batch_size, masked = False, benchmark= False)\n",
    "    val_loss, val_conloss, val_kl, val_risk = validate(factorVAE, valid_dataloader, batch_size, benchmark = False)\n",
    "    # val_loss, val_conloss, val_kl, val_risk = validate(factorVAE, test_dataloader, batch_size, benchmark = False)\n",
    "    pbar.set_postfix({\"Train Loss\": train_loss, \"Validation Loss\": val_loss})\n",
    "    save_root = os.path.join(save_dir, f'{run_name}_{num_factor}.pt')\n",
    "    torch.save(factorVAE.state_dict(), save_root)\n",
    "    train_loss_lst.append(train_loss)\n",
    "    valid_loss_lst.append(val_loss)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def list_to_df(loss_list):\n",
    "    tcl = pd.DataFrame([i[0] for i in loss_list])\n",
    "    vcl = pd.DataFrame([i[1] for i in loss_list])\n",
    "    return tcl, vcl"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "tcl, vcl = pd.DataFrame(train_loss_lst), pd.DataFrame(valid_loss_lst)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "tcl.to_csv('tcl.csv')\n",
    "vcl.to_csv('vcl.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "<AxesSubplot:>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tcl.plot()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "<AxesSubplot:>"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "vcl.plot()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [
    {
     "data": {
      "text/plain": "0.0"
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}