{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/jianming/PONet/program/project/FactorVAE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "from accelerate import Accelerator"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "num_epochs = 25\n",
    "lr = 0.0001\n",
    "batch_size = 512\n",
    "num_latent = 6 # feature size\n",
    "seq_len = 60\n",
    "num_factor = 64\n",
    "hidden_size = 64\n",
    "hidden_factor = 64\n",
    "seed = 42\n",
    "save_dir = r'/home/jianming/PONet/program/project/FactorVAE/data/best_models'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# train_df = pd.read_pickle(\"/home/jianming/PONet/program/project/FactorVAE/data/train_sp500_QLIB_False_NORM_False_CHAR_False_LEN_20.pkl\")\n",
    "# valid_df = pd.read_pickle(\"/home/jianming/PONet/program/project/FactorVAE/data/valid_sp500_QLIB_False_NORM_False_CHAR_False_LEN_20.pkl\")\n",
    "# test_df = pd.read_pickle(\"/home/jianming/PONet/program/project/FactorVAE/data/test_sp500_QLIB_False_NORM_False_CHAR_False_LEN_20.pkl\")\n",
    "# US_feature_dataset_market_sp500_valid_start2015-01-01_end2016-12-31_label5\n",
    "train_df = pd.read_pickle(\"/home/jianming/PONet/program/project/FactorVAE/data/US_feature_dataset_market_sp500_train_start2008-01-01_end2014-12-31_label1\")\n",
    "valid_df = pd.read_pickle(\"/home/jianming/PONet/program/project/FactorVAE/data/US_feature_dataset_market_sp500_valid_start2015-01-01_end2016-12-31_label1\")\n",
    "test_df = pd.read_pickle(\"/home/jianming/PONet/program/project/FactorVAE/data/US_feature_dataset_market_sp500_test_start2017-01-01_end2020-08-01_label1\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "                        CLOSE59   CLOSE58   CLOSE57   CLOSE56   CLOSE55  \\\ndatetime   instrument                                                     \n2010-01-04 A          -0.801470 -0.792347 -0.805499 -0.865277 -0.789545   \n           AA         -0.939941 -1.010894 -1.017612 -1.105897 -1.004793   \n           AAPL       -0.721489 -0.681588 -0.676562 -0.719535 -0.676018   \n           ABC        -1.130259 -1.073238 -1.041954 -1.094959 -0.954652   \n           ABT        -0.469372 -0.482687 -0.503474 -0.503206 -0.250961   \n...                         ...       ...       ...       ...       ...   \n2017-12-29 XYL        -0.202851 -0.197502 -0.190815 -0.199200 -0.248474   \n           YUM        -0.490197 -0.351213 -0.350005 -0.336936 -0.292932   \n           ZBH         0.177233  0.181779  0.049109  0.112167  0.126088   \n           ZION       -0.277664 -0.246040 -0.347252 -0.288158 -0.372653   \n           ZTS        -0.728868 -0.739231 -0.751547 -0.728709 -0.702852   \n\n                        CLOSE54   CLOSE53   CLOSE52   CLOSE51   CLOSE50  ...  \\\ndatetime   instrument                                                    ...   \n2010-01-04 A          -0.792142 -0.926031 -0.881090 -1.176907 -1.066803  ...   \n           AA         -0.995895 -1.188350 -1.186017 -1.361593 -1.361682  ...   \n           AAPL       -0.718102 -0.838528 -0.770304 -0.387089 -0.119082  ...   \n           ABC        -0.870498 -0.819805 -0.866395 -0.779866 -0.878006  ...   \n           ABT        -0.137176 -0.225172 -0.092749 -0.163453 -0.294764  ...   \n...                         ...       ...       ...       ...       ...  ...   \n2017-12-29 XYL        -0.178195 -0.207453 -0.041043 -0.426306 -0.371775  ...   \n           YUM        -0.326184 -0.357495 -0.363535 -0.374415 -0.402481  ...   \n           ZBH         0.274740  0.200915  0.183904  0.213350  0.271014  ...   \n           ZION       -0.482223 -0.512960 -0.486433 -0.645252 -0.622052  ...   \n           ZTS        -0.632751 -0.574249 -0.548104 -0.545166 -0.588158  ...   \n\n                        VOLUME9   VOLUME8   VOLUME7   VOLUME6   VOLUME5  \\\ndatetime   instrument                                                     \n2010-01-04 A           0.411700  0.298818 -0.209210 -0.420662 -1.603741   \n           AA          0.294136  2.791438  0.451646  0.003445 -0.998537   \n           AAPL        0.458316  0.476913 -0.632760 -0.662556  0.012183   \n           ABC         2.104521  0.139758 -0.390253  0.114148 -1.781350   \n           ABT         1.649119  0.253572 -0.155324 -1.024470 -1.570560   \n...                         ...       ...       ...       ...       ...   \n2017-12-29 XYL         1.498057  0.061790 -0.636312 -1.049967 -0.850350   \n           YUM         3.000000  0.647190 -0.203018 -0.321770  0.491947   \n           ZBH         2.270499  0.882518  3.000000  1.118766  0.122109   \n           ZION        3.000000  2.210243  0.105724  0.725600  0.580410   \n           ZTS         1.355472 -0.076837  0.280258  0.108470 -0.714964   \n\n                        VOLUME4   VOLUME3   VOLUME2   VOLUME1  VOLUME0  \ndatetime   instrument                                                   \n2010-01-04 A          -0.879167  0.303500  1.125745  1.020440      0.0  \n           AA         -0.611950 -1.061957 -0.739595 -1.068296      0.0  \n           AAPL        0.669912 -0.247133 -0.428418 -0.849569      0.0  \n           ABC        -1.233891 -0.973650 -0.862436 -1.182416      0.0  \n           ABT        -0.494855 -0.351178 -0.783187 -1.242245      0.0  \n...                         ...       ...       ...       ...      ...  \n2017-12-29 XYL        -1.539819 -1.857334 -1.269359 -1.614869      0.0  \n           YUM        -0.728033 -1.193908 -0.561972 -1.460446      0.0  \n           ZBH        -0.691201 -1.495628 -0.479991 -0.122525      0.0  \n           ZION       -0.695005 -0.854849 -1.130369 -0.678290      0.0  \n           ZTS        -0.487750 -1.263578 -0.808953 -1.696340      0.0  \n\n[865491 rows x 360 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>CLOSE59</th>\n      <th>CLOSE58</th>\n      <th>CLOSE57</th>\n      <th>CLOSE56</th>\n      <th>CLOSE55</th>\n      <th>CLOSE54</th>\n      <th>CLOSE53</th>\n      <th>CLOSE52</th>\n      <th>CLOSE51</th>\n      <th>CLOSE50</th>\n      <th>...</th>\n      <th>VOLUME9</th>\n      <th>VOLUME8</th>\n      <th>VOLUME7</th>\n      <th>VOLUME6</th>\n      <th>VOLUME5</th>\n      <th>VOLUME4</th>\n      <th>VOLUME3</th>\n      <th>VOLUME2</th>\n      <th>VOLUME1</th>\n      <th>VOLUME0</th>\n    </tr>\n    <tr>\n      <th>datetime</th>\n      <th>instrument</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">2010-01-04</th>\n      <th>A</th>\n      <td>-0.801470</td>\n      <td>-0.792347</td>\n      <td>-0.805499</td>\n      <td>-0.865277</td>\n      <td>-0.789545</td>\n      <td>-0.792142</td>\n      <td>-0.926031</td>\n      <td>-0.881090</td>\n      <td>-1.176907</td>\n      <td>-1.066803</td>\n      <td>...</td>\n      <td>0.411700</td>\n      <td>0.298818</td>\n      <td>-0.209210</td>\n      <td>-0.420662</td>\n      <td>-1.603741</td>\n      <td>-0.879167</td>\n      <td>0.303500</td>\n      <td>1.125745</td>\n      <td>1.020440</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>AA</th>\n      <td>-0.939941</td>\n      <td>-1.010894</td>\n      <td>-1.017612</td>\n      <td>-1.105897</td>\n      <td>-1.004793</td>\n      <td>-0.995895</td>\n      <td>-1.188350</td>\n      <td>-1.186017</td>\n      <td>-1.361593</td>\n      <td>-1.361682</td>\n      <td>...</td>\n      <td>0.294136</td>\n      <td>2.791438</td>\n      <td>0.451646</td>\n      <td>0.003445</td>\n      <td>-0.998537</td>\n      <td>-0.611950</td>\n      <td>-1.061957</td>\n      <td>-0.739595</td>\n      <td>-1.068296</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>AAPL</th>\n      <td>-0.721489</td>\n      <td>-0.681588</td>\n      <td>-0.676562</td>\n      <td>-0.719535</td>\n      <td>-0.676018</td>\n      <td>-0.718102</td>\n      <td>-0.838528</td>\n      <td>-0.770304</td>\n      <td>-0.387089</td>\n      <td>-0.119082</td>\n      <td>...</td>\n      <td>0.458316</td>\n      <td>0.476913</td>\n      <td>-0.632760</td>\n      <td>-0.662556</td>\n      <td>0.012183</td>\n      <td>0.669912</td>\n      <td>-0.247133</td>\n      <td>-0.428418</td>\n      <td>-0.849569</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>ABC</th>\n      <td>-1.130259</td>\n      <td>-1.073238</td>\n      <td>-1.041954</td>\n      <td>-1.094959</td>\n      <td>-0.954652</td>\n      <td>-0.870498</td>\n      <td>-0.819805</td>\n      <td>-0.866395</td>\n      <td>-0.779866</td>\n      <td>-0.878006</td>\n      <td>...</td>\n      <td>2.104521</td>\n      <td>0.139758</td>\n      <td>-0.390253</td>\n      <td>0.114148</td>\n      <td>-1.781350</td>\n      <td>-1.233891</td>\n      <td>-0.973650</td>\n      <td>-0.862436</td>\n      <td>-1.182416</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>ABT</th>\n      <td>-0.469372</td>\n      <td>-0.482687</td>\n      <td>-0.503474</td>\n      <td>-0.503206</td>\n      <td>-0.250961</td>\n      <td>-0.137176</td>\n      <td>-0.225172</td>\n      <td>-0.092749</td>\n      <td>-0.163453</td>\n      <td>-0.294764</td>\n      <td>...</td>\n      <td>1.649119</td>\n      <td>0.253572</td>\n      <td>-0.155324</td>\n      <td>-1.024470</td>\n      <td>-1.570560</td>\n      <td>-0.494855</td>\n      <td>-0.351178</td>\n      <td>-0.783187</td>\n      <td>-1.242245</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">2017-12-29</th>\n      <th>XYL</th>\n      <td>-0.202851</td>\n      <td>-0.197502</td>\n      <td>-0.190815</td>\n      <td>-0.199200</td>\n      <td>-0.248474</td>\n      <td>-0.178195</td>\n      <td>-0.207453</td>\n      <td>-0.041043</td>\n      <td>-0.426306</td>\n      <td>-0.371775</td>\n      <td>...</td>\n      <td>1.498057</td>\n      <td>0.061790</td>\n      <td>-0.636312</td>\n      <td>-1.049967</td>\n      <td>-0.850350</td>\n      <td>-1.539819</td>\n      <td>-1.857334</td>\n      <td>-1.269359</td>\n      <td>-1.614869</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>YUM</th>\n      <td>-0.490197</td>\n      <td>-0.351213</td>\n      <td>-0.350005</td>\n      <td>-0.336936</td>\n      <td>-0.292932</td>\n      <td>-0.326184</td>\n      <td>-0.357495</td>\n      <td>-0.363535</td>\n      <td>-0.374415</td>\n      <td>-0.402481</td>\n      <td>...</td>\n      <td>3.000000</td>\n      <td>0.647190</td>\n      <td>-0.203018</td>\n      <td>-0.321770</td>\n      <td>0.491947</td>\n      <td>-0.728033</td>\n      <td>-1.193908</td>\n      <td>-0.561972</td>\n      <td>-1.460446</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>ZBH</th>\n      <td>0.177233</td>\n      <td>0.181779</td>\n      <td>0.049109</td>\n      <td>0.112167</td>\n      <td>0.126088</td>\n      <td>0.274740</td>\n      <td>0.200915</td>\n      <td>0.183904</td>\n      <td>0.213350</td>\n      <td>0.271014</td>\n      <td>...</td>\n      <td>2.270499</td>\n      <td>0.882518</td>\n      <td>3.000000</td>\n      <td>1.118766</td>\n      <td>0.122109</td>\n      <td>-0.691201</td>\n      <td>-1.495628</td>\n      <td>-0.479991</td>\n      <td>-0.122525</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>ZION</th>\n      <td>-0.277664</td>\n      <td>-0.246040</td>\n      <td>-0.347252</td>\n      <td>-0.288158</td>\n      <td>-0.372653</td>\n      <td>-0.482223</td>\n      <td>-0.512960</td>\n      <td>-0.486433</td>\n      <td>-0.645252</td>\n      <td>-0.622052</td>\n      <td>...</td>\n      <td>3.000000</td>\n      <td>2.210243</td>\n      <td>0.105724</td>\n      <td>0.725600</td>\n      <td>0.580410</td>\n      <td>-0.695005</td>\n      <td>-0.854849</td>\n      <td>-1.130369</td>\n      <td>-0.678290</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>ZTS</th>\n      <td>-0.728868</td>\n      <td>-0.739231</td>\n      <td>-0.751547</td>\n      <td>-0.728709</td>\n      <td>-0.702852</td>\n      <td>-0.632751</td>\n      <td>-0.574249</td>\n      <td>-0.548104</td>\n      <td>-0.545166</td>\n      <td>-0.588158</td>\n      <td>...</td>\n      <td>1.355472</td>\n      <td>-0.076837</td>\n      <td>0.280258</td>\n      <td>0.108470</td>\n      <td>-0.714964</td>\n      <td>-0.487750</td>\n      <td>-1.263578</td>\n      <td>-0.808953</td>\n      <td>-1.696340</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>865491 rows × 360 columns</p>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['feature']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(\n",
    "        self,\n",
    "        df_feature,\n",
    "        df_label,\n",
    "        d_feat = 6,\n",
    "        batch_size=512,\n",
    "        shuffle=False,\n",
    "        device=None,\n",
    "    ):\n",
    "        assert len(df_feature) == len(df_label)\n",
    "        self.device = device\n",
    "\n",
    "        self.df_feature = df_feature\n",
    "        self.df_label = df_label\n",
    "\n",
    "        self.feature = torch.from_numpy(self.df_feature.values)\n",
    "        self.label = torch.from_numpy(self.df_label.values)\n",
    "\n",
    "        self.d_feat = d_feat\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label) // self.batch_size\n",
    "\n",
    "    def iter_batch(self):\n",
    "        indices = np.arange(len(self.label))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(indices)\n",
    "\n",
    "        for i in range(len(indices))[:: (self.batch_size)]:\n",
    "            if len(indices) - i < self.batch_size:\n",
    "                break\n",
    "            yield i, indices[i : i + self.batch_size]\n",
    "\n",
    "    def get_batch(self, i, slc):\n",
    "\n",
    "        outs = self.feature[slc].view(len(slc),self.d_feat, -1), self.label[slc]# No date in iter batch\n",
    "\n",
    "        return outs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_df['feature'].fillna(0),train_df['label'].fillna(0))\n",
    "valid_dataloader = DataLoader(valid_df['feature'].fillna(0),valid_df['label'].fillna(0))\n",
    "test_dataloader = DataLoader(test_df['feature'].fillna(0),test_df['label'].fillna(0))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, num_latent, hidden_size, num_layers=1):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        self.num_latent = num_latent\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.normalize = nn.LayerNorm(num_latent)\n",
    "        self.linear = nn.Linear(num_latent, num_latent)\n",
    "        self.leakyrelu = nn.LeakyReLU()\n",
    "        self.gru = nn.GRU(num_latent, hidden_size, num_layers, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #! x: (batch_size, seq_length, num_latent)\n",
    "        # Apply linear and LeakyReLU activation\n",
    "        #* layer norm\n",
    "        if x.shape[-1] != self.num_latent:\n",
    "            x = x.permute((0,2,1))\n",
    "        x = self.normalize(x)\n",
    "        out = self.linear(x)\n",
    "        out = self.leakyrelu(out)\n",
    "        # Forward propagate GRU\n",
    "        stock_latent, _ = self.gru(out)\n",
    "        return stock_latent[:,-1,:] #* stock_latent[-1]: (batch_size, hidden_size)\n",
    "\n",
    "class FactorEncoder(nn.Module):\n",
    "    def __init__(self, num_factors, num_portfolio, hidden_size):\n",
    "        super(FactorEncoder, self).__init__()\n",
    "        self.num_factors = num_factors\n",
    "        self.linear = nn.Linear(hidden_size, num_portfolio)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "        self.linear2 = nn.Linear(num_portfolio, num_factors)\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "    def mapping_layer(self, portfolio_return):\n",
    "        #! portfolio_return: (batch_size, 1)\n",
    "        #! mapping layer\n",
    "        # print(portfolio_return.shape)\n",
    "        mean = self.linear2(portfolio_return.squeeze(1))\n",
    "        sigma = self.softplus(mean)\n",
    "        return mean, sigma\n",
    "\n",
    "    def forward(self, stock_latent, returns):\n",
    "        #! stock_latent: (batch_size, hidden_size)\n",
    "        #! returns: (batch_size, 1)\n",
    "        #! make portfolio\n",
    "        weights = self.linear(stock_latent)\n",
    "        weights = self.softmax(weights) # (batch_size, num_portfolio)\n",
    "\n",
    "        # multiply weights and returns\n",
    "        #print(f\"weights shape: {weights.shape}, returns shape: {returns.shape}\") # [300, 20], [300, 1]\n",
    "        # check returns.shape is tuple\n",
    "        if returns.dim() == 1:\n",
    "            returns = returns.unsqueeze(1)\n",
    "        portfolio_return = torch.mm(weights.transpose(1,0), returns) #* portfolio_return: (M, 1)\n",
    "        #print(f\"portfolio_return shape: {portfolio_return.shape}\")\n",
    "\n",
    "        return self.mapping_layer(portfolio_return)\n",
    "\n",
    "class AlphaLayer(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(AlphaLayer, self).__init__()\n",
    "        self.linear1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.leakyrelu = nn.LeakyReLU()\n",
    "        self.mu_layer = nn.Linear(hidden_size, 1)\n",
    "        self.sigma_layer = nn.Linear(hidden_size, 1)\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "    def forward(self, stock_latent):\n",
    "        #* stock latent는 FeatureExtractor에서 나온 것 (batch_size, hidden_size)\n",
    "        stock_latent = self.linear1(stock_latent)\n",
    "        stock_latent = self.leakyrelu(stock_latent)\n",
    "        alpha_mu = self.mu_layer(stock_latent)\n",
    "        alpha_sigma = self.sigma_layer(stock_latent)\n",
    "        return alpha_mu, self.softplus(alpha_sigma)\n",
    "\n",
    "class BetaLayer(nn.Module):\n",
    "    \"\"\"calcuate factor exposure beta(N*K)\"\"\"\n",
    "    def __init__(self, hidden_size, num_factors):\n",
    "        super(BetaLayer, self).__init__()\n",
    "        self.linear1 = nn.Linear(hidden_size, num_factors)\n",
    "\n",
    "    def forward(self, stock_latent):\n",
    "        beta = self.linear1(stock_latent)\n",
    "        return beta\n",
    "\n",
    "class FactorDecoder(nn.Module):\n",
    "    def __init__(self, alpha_layer, beta_layer):\n",
    "        super(FactorDecoder, self).__init__()\n",
    "\n",
    "        self.alpha_layer = alpha_layer\n",
    "        self.beta_layer = beta_layer\n",
    "\n",
    "    def reparameterize(self, mu, sigma):\n",
    "        eps = torch.randn_like(sigma)\n",
    "        return mu + eps * sigma\n",
    "\n",
    "    def forward(self, stock_latent, factor_mu, factor_sigma):\n",
    "        #! warning: alpha_mu, alpha_sigma -> (N), (N)\n",
    "        alpha_mu, alpha_sigma = self.alpha_layer(stock_latent)\n",
    "        #print(f\"alpha_mu shape: {alpha_mu.shape}, alpha_sigma shape: {alpha_sigma.shape}\")\n",
    "        beta = self.beta_layer(stock_latent)\n",
    "\n",
    "        factor_mu = factor_mu.view(-1, 1)\n",
    "        factor_sigma = factor_sigma.view(-1, 1)\n",
    "\n",
    "        # Replace any zero values in factor_sigma with a small value\n",
    "        factor_sigma[factor_sigma == 0] = 1e-6\n",
    "        #print(f\"factor_mu shape: {factor_mu.shape}, factor_sigma shape: {factor_sigma.shape}\")\n",
    "        #print(f\"beta shape: {beta.shape}\")\n",
    "        mu = alpha_mu + torch.matmul(beta, factor_mu)\n",
    "        sigma = torch.sqrt(alpha_sigma**2 + torch.matmul(beta**2, factor_sigma**2) + 1e-6)\n",
    "\n",
    "        return self.reparameterize(mu, sigma)\n",
    "\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "\n",
    "        self.query = nn.Parameter(torch.randn(hidden_size))\n",
    "        self.key_layer = nn.Linear(hidden_size, hidden_size)\n",
    "        self.value_layer = nn.Linear(hidden_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, stock_latent):\n",
    "        #* calculate attention weights\n",
    "\n",
    "        self.key = self.key_layer(stock_latent)\n",
    "        self.value = self.value_layer(stock_latent)\n",
    "\n",
    "        attention_weights = torch.matmul(self.query, self.key.transpose(1,0)) # (N)\n",
    "        #* scaling\n",
    "        attention_weights = attention_weights / torch.sqrt(torch.tensor(self.key.shape[0])+ 1e-6)\n",
    "        # print(f\"attention_weights shape: {attention_weights.shape}\")\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        attention_weights = F.relu(attention_weights) # max(0, x)\n",
    "        attention_weights = F.softmax(attention_weights, dim=0) # (N)\n",
    "\n",
    "        #! calculate context vector\n",
    "        if torch.isnan(attention_weights).any() or torch.isinf(attention_weights).any():\n",
    "            return torch.zeros_like(self.value[0])\n",
    "        else:\n",
    "            context_vector = torch.matmul(attention_weights, self.value) # (H)\n",
    "            return context_vector\n",
    "\n",
    "class FactorPredictor(nn.Module):\n",
    "    def __init__(self, batch_size, hidden_size, num_factor):\n",
    "        super(FactorPredictor, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        self.num_factor = num_factor\n",
    "        self.attention_layers = nn.ModuleList([AttentionLayer(self.hidden_size) for _ in range(num_factor)])\n",
    "\n",
    "        self.linear = nn.Linear(hidden_size, hidden_size)\n",
    "        self.leakyrelu = nn.LeakyReLU()\n",
    "        self.mu_layer = nn.Linear(hidden_size, 1)\n",
    "        self.sigma_layer = nn.Linear(hidden_size, 1)\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "    def forward(self, stock_latent):\n",
    "        for i in range(self.num_factor):\n",
    "            attention_layer = self.attention_layers[i](stock_latent)\n",
    "            if i == 0:\n",
    "                h_multi = attention_layer\n",
    "            else:\n",
    "                h_multi = torch.cat((h_multi, attention_layer), dim=0)\n",
    "        h_multi = h_multi.view(self.num_factor, -1)\n",
    "\n",
    "        # print(\"h_multi:\", h_multi.shape)\n",
    "        h_multi = self.linear(h_multi)\n",
    "        h_multi = self.leakyrelu(h_multi)\n",
    "        pred_mu = self.mu_layer(h_multi)\n",
    "        pred_sigma = self.sigma_layer(h_multi)\n",
    "        pred_sigma = self.softplus(pred_sigma)\n",
    "        pred_mu = pred_mu.view(-1)\n",
    "        pred_sigma = pred_sigma.view(-1)\n",
    "        return pred_mu, pred_sigma\n",
    "\n",
    "class FactorVAE(nn.Module):\n",
    "    def __init__(self, feature_extractor, factor_encoder, factor_decoder, factor_predictor, risk_extrator, hidden_factor):\n",
    "        super(FactorVAE, self).__init__()\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.factor_encoder = factor_encoder\n",
    "        self.factor_decoder = factor_decoder\n",
    "        self.factor_predictor = factor_predictor\n",
    "        self.risk_extrator = risk_extrator\n",
    "        self.hidden_factor = hidden_factor\n",
    "        self.map = nn.Linear(128,hidden_factor)\n",
    "\n",
    "    @staticmethod\n",
    "    def KL_Divergence(mu1, sigma1, mu2, sigma2):\n",
    "        #! mu1, mu2: (batch_size, 1)\n",
    "        #! sigma1, sigma2: (batch_size, 1)\n",
    "        #! output: (batch_size, 1)\n",
    "        kl_div = (torch.log(sigma2/ sigma1) + (sigma1**2 + (mu1 - mu2)**2) / (2 * sigma2**2) - 0.5).sum()\n",
    "        return kl_div\n",
    "\n",
    "    def forward(self, x, returns):\n",
    "        #! x: (batch_size, seq_length, num_latent)\n",
    "        #! returns: (batch_size, 1)\n",
    "\n",
    "        stock_latent = self.feature_extractor(x)\n",
    "        risk_latent = self.risk_extrator(x)\n",
    "        stock_latent = torch.cat([stock_latent,risk_latent],dim=1)\n",
    "        stock_latent = self.map(stock_latent)\n",
    "        # corr = torch.corrcoef(stock_latent.transpose(1,0))\n",
    "        # U,_,_ = torch.pca_lowrank(corr, q=self.hidden_factor, center=True, niter=2)\n",
    "        # stock_latent = torch.mm(stock_latent,U)/torch.sqrt(torch.Tensor(len(stock_latent)))\n",
    "\n",
    "        factor_mu, factor_sigma = self.factor_encoder(stock_latent, returns)\n",
    "        reconstruction = self.factor_decoder(stock_latent, factor_mu, factor_sigma)\n",
    "        pred_mu, pred_sigma = self.factor_predictor(stock_latent)\n",
    "\n",
    "        # print(f\"pred_mu: {pred_mu.shape}, pred_sigma: {pred_sigma.shape}\")\n",
    "        # Define VAE loss function with reconstruction loss and KL divergence\n",
    "        #* Some adjustment\n",
    "        #* stock_adj: number of stocks that have no return data\n",
    "        stock_adj = 0\n",
    "        for i in range(len(returns)-1,-1,-1):\n",
    "            if returns[i] == 0:\n",
    "                stock_adj += 1\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        if stock_adj > 0:\n",
    "            reconstruction_loss = F.mse_loss(reconstruction[:-stock_adj], returns[:-stock_adj])\n",
    "        else:\n",
    "            reconstruction_loss = F.mse_loss(reconstruction, returns)\n",
    "\n",
    "        # Calculate KL divergence between two Gaussian distributions\n",
    "        if torch.any(pred_sigma == 0):\n",
    "            pred_sigma[pred_sigma == 0] = 1e-6\n",
    "        kl_divergence = self.KL_Divergence(factor_mu, factor_sigma, pred_mu, pred_sigma)\n",
    "\n",
    "        vae_loss = reconstruction_loss + kl_divergence\n",
    "        # print(\"loss: \", vae_loss)\n",
    "        return vae_loss, reconstruction, factor_mu, factor_sigma, pred_mu, pred_sigma #! reconstruction, factor_mu, factor_sigma\n",
    "\n",
    "    # 학습 이후 사용\n",
    "    def prediction(self, x):\n",
    "        stock_latent = self.feature_extractor(x)\n",
    "        pred_mu, pred_sigma = self.factor_predictor(stock_latent)\n",
    "        y_pred = self.factor_decoder(stock_latent, pred_mu, pred_sigma)\n",
    "        return y_pred\n",
    "\n",
    "    def latent_factor(self, x):\n",
    "        stock_latent = self.feature_extractor(x)\n",
    "        pred_mu, pred_sigma = self.factor_predictor(stock_latent)\n",
    "        return (pred_mu, pred_sigma)\n",
    "\n",
    "class GATModel(nn.Module):\n",
    "    def __init__(self, d_feat=158, hidden_size=64, num_layers=2, dropout=0.0, base_model=\"GRU\"):\n",
    "        super().__init__()\n",
    "\n",
    "        if base_model == \"GRU\":\n",
    "            self.rnn = nn.GRU(\n",
    "                input_size=d_feat,\n",
    "                hidden_size=hidden_size,\n",
    "                num_layers=num_layers,\n",
    "                batch_first=True,\n",
    "                dropout=dropout,\n",
    "            )\n",
    "        elif base_model == \"LSTM\":\n",
    "            self.rnn = nn.LSTM(\n",
    "                input_size=d_feat,\n",
    "                hidden_size=hidden_size,\n",
    "                num_layers=num_layers,\n",
    "                batch_first=True,\n",
    "                dropout=dropout,\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"unknown base model name `%s`\" % base_model)\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.d_feat = d_feat\n",
    "        self.transformation = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.a = nn.Parameter(torch.randn(self.hidden_size * 2, 1))\n",
    "        self.a.requires_grad = True\n",
    "        self.fc = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.fc_out = nn.Linear(hidden_size, hidden_size)\n",
    "        self.leaky_relu = nn.LeakyReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def cal_attention(self, x, y):\n",
    "        x = self.transformation(x)\n",
    "        y = self.transformation(y)\n",
    "\n",
    "        sample_num = x.shape[0]\n",
    "        dim = x.shape[1]\n",
    "        e_x = x.expand(sample_num, sample_num, dim)\n",
    "        e_y = torch.transpose(e_x, 0, 1)\n",
    "        attention_in = torch.cat((e_x, e_y), 2).view(-1, dim * 2)\n",
    "        self.a_t = torch.t(self.a)\n",
    "        attention_out = self.a_t.mm(torch.t(attention_in)).view(sample_num, sample_num)\n",
    "        attention_out = self.leaky_relu(attention_out)\n",
    "        att_weight = self.softmax(attention_out)\n",
    "        return att_weight\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [N, F*T]\n",
    "        x = x.reshape(len(x), self.d_feat, -1)  # [N, F, T]\n",
    "        x = x.permute(0, 2, 1)  # [N, T, F]\n",
    "        out, _ = self.rnn(x)\n",
    "        hidden = out[:, -1, :]\n",
    "        att_weight = self.cal_attention(hidden, hidden)\n",
    "        hidden = att_weight.mm(hidden) + hidden\n",
    "        hidden = self.fc(hidden)\n",
    "        hidden = self.leaky_relu(hidden)\n",
    "        return self.fc_out(hidden).squeeze() # [N, hidden size]\n",
    "\n",
    "class RiskFeatureExtractor(nn.Module):\n",
    "    \"\"\"supervise variance-covariance matrix reconstuction\"\"\"\n",
    "    def __init__(self, num_latent, hidden_size,num_layers=2,dropout=0.2,base_model = 'GRU'):\n",
    "        super(RiskFeatureExtractor, self).__init__()\n",
    "        self.num_latent = num_latent\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.normalize = nn.LayerNorm(self.num_latent)\n",
    "        self.linear = nn.Linear(self.num_latent, self.num_latent)\n",
    "        self.leakyrelu = nn.LeakyReLU()\n",
    "        self.GAT_model = GATModel(\n",
    "            d_feat=self.num_latent,\n",
    "            hidden_size=self.hidden_size,\n",
    "            num_layers=num_layers ,\n",
    "            dropout=dropout,\n",
    "            base_model=base_model,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.shape[-1] != self.num_latent:\n",
    "            x = x.permute((0,2,1))\n",
    "        x = self.normalize(x)\n",
    "        out = self.linear(x)\n",
    "        out = self.leakyrelu(out)\n",
    "        risk_latent = self.GAT_model(out)\n",
    "        return risk_latent #* stock_latent[-1]: (batch_size, hidden_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# model\n",
    "class GATModel(nn.Module):\n",
    "    def __init__(self, d_feat=158, hidden_size=64, num_layers=2, dropout=0.0, base_model=\"GRU\"):\n",
    "        super().__init__()\n",
    "\n",
    "        if base_model == \"GRU\":\n",
    "            self.rnn = nn.GRU(\n",
    "                input_size=d_feat,\n",
    "                hidden_size=hidden_size,\n",
    "                num_layers=num_layers,\n",
    "                batch_first=True,\n",
    "                dropout=dropout,\n",
    "            )\n",
    "        elif base_model == \"LSTM\":\n",
    "            self.rnn = nn.LSTM(\n",
    "                input_size=d_feat,\n",
    "                hidden_size=hidden_size,\n",
    "                num_layers=num_layers,\n",
    "                batch_first=True,\n",
    "                dropout=dropout,\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"unknown base model name `%s`\" % base_model)\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.d_feat = d_feat\n",
    "        self.transformation = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.a = nn.Parameter(torch.randn(self.hidden_size * 2, 1))\n",
    "        self.a.requires_grad = True\n",
    "        self.fc = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.fc_out = nn.Linear(hidden_size, hidden_size)\n",
    "        self.leaky_relu = nn.LeakyReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def cal_attention(self, x, y):\n",
    "        x = self.transformation(x)\n",
    "        y = self.transformation(y)\n",
    "\n",
    "        sample_num = x.shape[0]\n",
    "        dim = x.shape[1]\n",
    "        e_x = x.expand(sample_num, sample_num, dim)\n",
    "        e_y = torch.transpose(e_x, 0, 1)\n",
    "        attention_in = torch.cat((e_x, e_y), 2).view(-1, dim * 2)\n",
    "        self.a_t = torch.t(self.a)\n",
    "        attention_out = self.a_t.mm(torch.t(attention_in)).view(sample_num, sample_num)\n",
    "        attention_out = self.leaky_relu(attention_out)\n",
    "        att_weight = self.softmax(attention_out)\n",
    "        return att_weight\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [N, F*T]\n",
    "        x = x.reshape(len(x), self.d_feat, -1)  # [N, F, T]\n",
    "        x = x.permute(0, 2, 1)  # [N, T, F]\n",
    "        out, _ = self.rnn(x)\n",
    "        hidden = out[:, -1, :]\n",
    "        att_weight = self.cal_attention(hidden, hidden)\n",
    "        hidden = att_weight.mm(hidden) + hidden\n",
    "        hidden = self.fc(hidden)\n",
    "        hidden = self.leaky_relu(hidden)\n",
    "        return self.fc_out(hidden).squeeze() # [N, hidden size]\n",
    "\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, num_latent, hidden_size, num_layers=1):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        self.num_latent = num_latent\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.normalize = nn.LayerNorm(num_latent)\n",
    "        self.linear = nn.Linear(num_latent, num_latent)\n",
    "        self.leakyrelu = nn.LeakyReLU()\n",
    "        self.gru = nn.GRU(num_latent, hidden_size, num_layers, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.shape[-1] != self.num_latent:\n",
    "            x = x.permute((0,2,1))\n",
    "        x = self.normalize(x)\n",
    "        out = self.linear(x)\n",
    "        out = self.leakyrelu(out)\n",
    "        # Forward propagate GRU\n",
    "        stock_latent, _ = self.gru(out)\n",
    "        return stock_latent[:,-1,:] #* stock_latent[-1]: (batch_size, hidden_size)\n",
    "\n",
    "class FactorEncoder(nn.Module):\n",
    "    def __init__(self, num_factors, num_portfolio, hidden_size):\n",
    "        super(FactorEncoder, self).__init__()\n",
    "        self.num_factors = num_factors\n",
    "        self.linear = nn.Linear(hidden_size, num_portfolio)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "        self.linear2 = nn.Linear(num_portfolio, num_factors)\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "    def mapping_layer(self, portfolio_return):\n",
    "        #! portfolio_return: (batch_size, 1)\n",
    "        #! mapping layer\n",
    "        # print(portfolio_return.shape)\n",
    "        mean = self.linear2(portfolio_return.squeeze(1))\n",
    "        sigma = self.softplus(mean)\n",
    "        return mean, sigma\n",
    "\n",
    "    def forward(self, stock_latent, returns):\n",
    "        weights = self.linear(stock_latent)\n",
    "        weights = self.softmax(weights) # (batch_size, num_portfolio)\n",
    "\n",
    "        if returns.dim() == 1:\n",
    "            returns = returns.unsqueeze(1)\n",
    "        portfolio_return = torch.mm(weights.transpose(1,0), returns) #* portfolio_return: (M, 1)\n",
    "\n",
    "        #print(f\"portfolio_return shape: {portfolio_return.shape}\")\n",
    "\n",
    "        return self.mapping_layer(portfolio_return)\n",
    "\n",
    "class AlphaLayer(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(AlphaLayer, self).__init__()\n",
    "        self.linear1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.leakyrelu = nn.LeakyReLU()\n",
    "        self.mu_layer = nn.Linear(hidden_size, 1)\n",
    "        self.sigma_layer = nn.Linear(hidden_size, 1)\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "    def forward(self, stock_latent):\n",
    "        #* stock latent는 FeatureExtractor에서 나온 것 (batch_size, hidden_size)\n",
    "        stock_latent = self.linear1(stock_latent)\n",
    "        stock_latent = self.leakyrelu(stock_latent)\n",
    "        alpha_mu = self.mu_layer(stock_latent)\n",
    "        alpha_sigma = self.sigma_layer(stock_latent)\n",
    "        return alpha_mu, self.softplus(alpha_sigma)\n",
    "\n",
    "class BetaLayer(nn.Module):\n",
    "    \"\"\"calcuate factor exposure beta(N*K)\"\"\"\n",
    "    def __init__(self, hidden_size, num_factors):\n",
    "        super(BetaLayer, self).__init__()\n",
    "        self.linear1 = nn.Linear(hidden_size, num_factors)\n",
    "\n",
    "    def forward(self, stock_latent):\n",
    "        beta = self.linear1(stock_latent)\n",
    "        return beta\n",
    "\n",
    "class RiskFeatureExtractor(nn.Module):\n",
    "    \"\"\"supervise variance-covariance matrix reconstuction\"\"\"\n",
    "    def __init__(self, num_latent, hidden_size,num_layers=2,dropout=0.2,base_model = 'GRU'):\n",
    "        super(RiskFeatureExtractor, self).__init__()\n",
    "        self.num_latent = num_latent\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.normalize = nn.LayerNorm(self.num_latent)\n",
    "        self.linear = nn.Linear(self.num_latent, self.num_latent)\n",
    "        self.leakyrelu = nn.LeakyReLU()\n",
    "        self.GAT_model = GATModel(\n",
    "            d_feat=self.num_latent,\n",
    "            hidden_size=self.hidden_size,\n",
    "            num_layers=num_layers ,\n",
    "            dropout=dropout,\n",
    "            base_model=base_model,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.shape[-1] != self.num_latent:\n",
    "            x = x.permute((0,2,1))\n",
    "        x = self.normalize(x)\n",
    "        out = self.linear(x)\n",
    "        out = self.leakyrelu(out)\n",
    "        risk_latent = self.GAT_model(out)\n",
    "        return risk_latent #* stock_latent[-1]: (batch_size, hidden_size)\n",
    "\n",
    "class FactorDecoder(nn.Module):\n",
    "    def __init__(self, alpha_layer, beta_layer):\n",
    "        super(FactorDecoder, self).__init__()\n",
    "\n",
    "        self.alpha_layer = alpha_layer\n",
    "        self.beta_layer = beta_layer\n",
    "        self.sigma_layer = nn.Linear(hidden_size, 1)\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "    def reparameterize(self, mu, sigma):\n",
    "        eps = torch.randn_like(sigma)\n",
    "        return mu + eps * sigma\n",
    "\n",
    "    def forward(self, stock_latent, factor_mu, factor_sigma):\n",
    "        #! warning: alpha_mu, alpha_sigma -> (N), (N)\n",
    "        alpha_mu, alpha_sigma = self.alpha_layer(stock_latent)\n",
    "        #print(f\"alpha_mu shape: {alpha_mu.shape}, alpha_sigma shape: {alpha_sigma.shape}\")\n",
    "        beta = self.beta_layer(stock_latent)\n",
    "\n",
    "        factor_mu = factor_mu.view(-1, 1)\n",
    "        factor_sigma = factor_sigma.view(-1, 1)\n",
    "\n",
    "        # Replace any zero values in factor_sigma with a small value\n",
    "        factor_sigma[factor_sigma == 0] = 1e-6\n",
    "        #print(f\"factor_mu shape: {factor_mu.shape}, factor_sigma shape: {factor_sigma.shape}\")\n",
    "        #print(f\"beta shape: {beta.shape}\")\n",
    "        mu = alpha_mu + torch.matmul(beta, factor_mu)\n",
    "        sigma = torch.sqrt(alpha_sigma**2 + torch.matmul(beta**2, factor_sigma**2) + 1e-6)\n",
    "        return self.reparameterize(mu, sigma), sigma**2\n",
    "\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "\n",
    "        self.query = nn.Parameter(torch.randn(hidden_size))\n",
    "        self.key_layer = nn.Linear(hidden_size, hidden_size)\n",
    "        self.value_layer = nn.Linear(hidden_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, stock_latent):\n",
    "        #* calculate attention weights\n",
    "\n",
    "        self.key = self.key_layer(stock_latent)\n",
    "        self.value = self.value_layer(stock_latent)\n",
    "\n",
    "        attention_weights = torch.matmul(self.query, self.key.transpose(1,0)) # (N)\n",
    "        #* scaling\n",
    "        attention_weights = attention_weights / torch.sqrt(torch.tensor(self.key.shape[0])+ 1e-6)\n",
    "        # print(f\"attention_weights shape: {attention_weights.shape}\")\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        attention_weights = F.relu(attention_weights) # max(0, x)\n",
    "        attention_weights = F.softmax(attention_weights, dim=0) # (N)\n",
    "\n",
    "        #! calculate context vector\n",
    "        if torch.isnan(attention_weights).any() or torch.isinf(attention_weights).any():\n",
    "            return torch.zeros_like(self.value[0])\n",
    "        else:\n",
    "            context_vector = torch.matmul(attention_weights, self.value) # (H)\n",
    "            return context_vector\n",
    "\n",
    "class FactorPredictor(nn.Module):\n",
    "    def __init__(self, batch_size, hidden_size, num_factor):\n",
    "        super(FactorPredictor, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        self.num_factor = num_factor\n",
    "        self.attention_layers = nn.ModuleList([AttentionLayer(self.hidden_size) for _ in range(num_factor)])\n",
    "\n",
    "        self.linear = nn.Linear(hidden_size, hidden_size)\n",
    "        self.leakyrelu = nn.LeakyReLU()\n",
    "        self.mu_layer = nn.Linear(hidden_size, 1)\n",
    "        self.sigma_layer = nn.Linear(hidden_size, 1)\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "    def forward(self, stock_latent):\n",
    "        for i in range(self.num_factor):\n",
    "            attention_layer = self.attention_layers[i](stock_latent)\n",
    "            if i == 0:\n",
    "                h_multi = attention_layer\n",
    "            else:\n",
    "                h_multi = torch.cat((h_multi, attention_layer), dim=0)\n",
    "        h_multi = h_multi.view(self.num_factor, -1)\n",
    "\n",
    "        # print(\"h_multi:\", h_multi.shape)\n",
    "        h_multi = self.linear(h_multi)\n",
    "        h_multi = self.leakyrelu(h_multi)\n",
    "        pred_mu = self.mu_layer(h_multi)\n",
    "        pred_sigma = self.sigma_layer(h_multi)\n",
    "        pred_sigma = self.softplus(pred_sigma)\n",
    "        pred_mu = pred_mu.view(-1)\n",
    "        pred_sigma = pred_sigma.view(-1)\n",
    "        return pred_mu, pred_sigma\n",
    "\n",
    "class FactorVAE(nn.Module):\n",
    "    def __init__(self, feature_extractor, factor_encoder, factor_decoder, factor_predictor, risk_extrator, hidden_size_decoder, num_latent):\n",
    "        super(FactorVAE, self).__init__()\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.factor_encoder = factor_encoder\n",
    "        self.factor_decoder = factor_decoder\n",
    "        self.factor_predictor = factor_predictor\n",
    "        self.risk_extrator = risk_extrator\n",
    "        self.hidden_size_decoder = hidden_size_decoder\n",
    "\n",
    "    @staticmethod\n",
    "    def KL_Divergence(mu1, sigma1, mu2, sigma2):\n",
    "        #! mu1, mu2: (batch_size, 1)\n",
    "        #! sigma1, sigma2: (batch_size, 1)\n",
    "        #! output: (batch_size, 1)\n",
    "        kl_div = (torch.log(sigma2/ sigma1) + (sigma1**2 + (mu1 - mu2)**2) / (2 * sigma2**2) - 0.5).sum()\n",
    "        return kl_div\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        #! x: (batch_size, seq_length, num_latent)\n",
    "        #! returns: (batch_size, 1)\n",
    "        x = F.normalize(x)\n",
    "        variance = torch.diag(torch.mm(labels,labels.transpose(1,0)))\n",
    "        variance = variance / variance.mean()\n",
    "        variance = variance.unsqueeze(1)\n",
    "\n",
    "        returns = labels[:,-1].reshape(-1,1)\n",
    "        stock_latent = self.feature_extractor(x)\n",
    "        risk_latent = self.risk_extrator(x)\n",
    "        stock_latent = torch.cat([stock_latent,risk_latent],dim=1)\n",
    "        corr = torch.corrcoef(stock_latent.transpose(1,0))\n",
    "        U,S,V = torch.pca_lowrank(corr, q=hidden_size_decoder, center=True, niter=2)\n",
    "        # U feature size * q\n",
    "        stock_latent = torch.mm(stock_latent, U)\n",
    "        factor_mu, factor_sigma = self.factor_encoder(stock_latent, returns)\n",
    "        reconstruction, covariance = self.factor_decoder(stock_latent, factor_mu, factor_sigma)\n",
    "        covariance = covariance / covariance.mean()\n",
    "        pred_mu, pred_sigma = self.factor_predictor(stock_latent)\n",
    "        stock_adj = 0\n",
    "        for i in range(len(returns)-1,-1,-1):\n",
    "            if returns[i] == 0:\n",
    "                stock_adj += 1\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        if stock_adj > 0:\n",
    "            reconstruction_loss = F.mse_loss(reconstruction[:-stock_adj], returns[:-stock_adj])\n",
    "            risk_reconstruction = F.mse_loss(covariance[:-stock_adj,:-stock_adj], variance[:-stock_adj,:-stock_adj])\n",
    "        else:\n",
    "            reconstruction_loss = F.mse_loss(reconstruction, returns)\n",
    "            risk_reconstruction = F.mse_loss(covariance, variance)\n",
    "\n",
    "        # Calculate KL divergence between two Gaussian distributions\n",
    "        if torch.any(pred_sigma == 0):\n",
    "            pred_sigma[pred_sigma == 0] = 1e-6\n",
    "        kl_divergence = self.KL_Divergence(factor_mu, factor_sigma, pred_mu, pred_sigma)\n",
    "\n",
    "        vae_loss = reconstruction_loss + kl_divergence# + risk_reconstruction\n",
    "        # print(\"loss: \", vae_loss)\n",
    "        return vae_loss, reconstruction, factor_mu, factor_sigma, pred_mu, pred_sigma,risk_reconstruction  #! reconstruction, factor_mu, factor_sigma\n",
    "\n",
    "    def prediction(self, x):\n",
    "        stock_latent = self.feature_extractor(x)\n",
    "        pred_mu, pred_sigma = self.factor_predictor(stock_latent)\n",
    "        y_pred,_ = self.factor_decoder(stock_latent, pred_mu, pred_sigma)\n",
    "        return y_pred\n",
    "\n",
    "    def latent_factor(self, x):\n",
    "        stock_latent = self.feature_extractor(x)\n",
    "        pred_mu, pred_sigma = self.factor_predictor(stock_latent)\n",
    "        return (pred_mu, pred_sigma)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "set_seed(seed)\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "# create model\n",
    "feature_extractor = FeatureExtractor(num_latent=num_latent, hidden_size=hidden_size)\n",
    "risk_extrator = RiskFeatureExtractor(num_latent=num_latent, hidden_size = hidden_size)\n",
    "factor_encoder = FactorEncoder(num_factors=num_factor, num_portfolio=num_factor, hidden_size=hidden_factor)\n",
    "alpha_layer = AlphaLayer(hidden_factor)\n",
    "beta_layer = BetaLayer(hidden_factor, num_factor)\n",
    "factor_decoder = FactorDecoder(alpha_layer, beta_layer)\n",
    "factor_predictor = FactorPredictor(batch_size, hidden_factor, num_factor)\n",
    "factorVAE = FactorVAE(feature_extractor, factor_encoder, factor_decoder, factor_predictor, risk_extrator, hidden_factor)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "def train(factor_model, dataloader, optimizer, batch_size, masked = False):\n",
    "    factor_model.to(device)\n",
    "    factor_model.train()\n",
    "    total_loss = 0\n",
    "    reconstruction_loss = 0\n",
    "    kl_loss = 0\n",
    "    risk_loss = 0\n",
    "    mask_id = torch.randint(0, 500, (30,))\n",
    "    pbar = tqdm(range(len(dataloader)), desc=\"Training\", position=0,dynamic_ncols = True)\n",
    "    for (i, slc),_ in zip(dataloader.iter_batch(),pbar):\n",
    "        char, returns = dataloader.get_batch(i, slc)\n",
    "        if char.size(0)!=batch_size:\n",
    "            continue\n",
    "        inputs = char\n",
    "        labels = returns\n",
    "        inputs = inputs.float().to(device)\n",
    "        labels = labels.float().to(device)\n",
    "\n",
    "        if masked:\n",
    "            inputs[mask_id,:,:] = 0\n",
    "\n",
    "        if torch.isnan(inputs).any() or torch.isnan(labels).any():\n",
    "            continue\n",
    "        optimizer.zero_grad()\n",
    "        loss, reconstruction, factor_mu, factor_sigma, pred_mu, pred_sigma = factor_model(inputs, labels)\n",
    "\n",
    "        if loss>10:\n",
    "            continue\n",
    "        total_loss += loss.item() * inputs.size(0)\n",
    "        # loss.backward()\n",
    "\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "        nn.utils.clip_grad_norm_(factor_model.parameters(), 1.0)\n",
    "        reconstruction_loss += reconstruction.squeeze().mean().item()\n",
    "        kl_loss += loss.item() + reconstruction.squeeze().mean().item()\n",
    "        # risk_loss += risk.squeeze().mean().item()\n",
    "        pbar.set_postfix({\"Train Loss\": loss.item()})\n",
    "\n",
    "        # print(loss)\n",
    "    avg_loss = total_loss / len(dataloader) / batch_size\n",
    "    reconstruction_loss = reconstruction_loss/len(dataloader) /batch_size\n",
    "    kl_loss = kl_loss / len(dataloader) /batch_size\n",
    "    return avg_loss, reconstruction_loss, kl_loss, risk_loss\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(factor_model, dataloader, batch_size):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    factor_model.to(device)\n",
    "    factor_model.eval()\n",
    "    total_loss = 0\n",
    "    reconstruction_loss = 0\n",
    "    kl_loss = 0\n",
    "    risk_loss = 0\n",
    "    for i, slc in dataloader.iter_batch():\n",
    "        char, returns = dataloader.get_batch(i, slc)\n",
    "        if char.size(0)!=batch_size:\n",
    "            continue\n",
    "        inputs = char\n",
    "        labels = returns[:,-1].reshape(-1,1)\n",
    "        inputs = inputs.float().to(device)\n",
    "        labels = labels.float().to(device)\n",
    "\n",
    "        loss, reconstruction, factor_mu, factor_sigma, pred_mu, pred_sigma = factor_model(inputs, labels)\n",
    "        total_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        reconstruction_loss += reconstruction.squeeze().mean().item()\n",
    "        # risk_loss += risk.squeeze().mean().item()\n",
    "        kl_loss += loss.item() + reconstruction.squeeze().mean().item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader) /batch_size\n",
    "    reconstruction_loss = reconstruction_loss/len(dataloader) / batch_size\n",
    "    kl_loss = kl_loss / len(dataloader) / batch_size\n",
    "    return avg_loss, reconstruction_loss, kl_loss, risk_loss\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(factor_model, dataloader, seq_len):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    factor_model.to(device)\n",
    "    factor_model.eval()\n",
    "    total_loss = 0\n",
    "    with tqdm(total=len(dataloader)-seq_len+1) as pbar:\n",
    "        for char, returns,idx in dataloader:\n",
    "            if char.shape[1] != seq_len:\n",
    "                continue\n",
    "            inputs = char.to(device)\n",
    "            labels = returns.to(device)\n",
    "            inputs = inputs.float()\n",
    "            labels = labels.float()\n",
    "\n",
    "            loss, reconstruction, factor_mu, factor_sigma, pred_mu, pred_sigma = factor_model(inputs, labels)\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "            pbar.update(1)\n",
    "    avg_loss = total_loss / len(dataloader.dataset)\n",
    "    return avg_loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "optimizer = torch.optim.Adam(factorVAE.parameters(), lr=0.001)\n",
    "accelerator = Accelerator()\n",
    "# model_name = '/home/jianming/PONet/program/project/FactorVAE/data/best_models/sp500model_without_riskmodel_period5_64.pt'\n",
    "# factorVAE.load_state_dict(torch.load(model_name))\n",
    "train_dataloader, factorVAE, optimizer = accelerator.prepare(train_dataloader, factorVAE, optimizer)\n",
    "# optimizer = torch.optim.SGD(factorVAE.parameters(), lr=0.0001, momentum=0.9)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "data": {
      "text/plain": "Training:   0%|                                                                                               …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fbeb07d2f9eb454aa96439ffb50d7d9e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Training:   0%|                                                                                               …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bcd78ab8a4aa42578dec0cfb91245730"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Training:   0%|                                                                                               …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ed9176d7d5f242dea2c4433fd8ad65fc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Training:   0%|                                                                                               …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bf99cd35a93c4020bc5d71dfcb7f7a10"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Training:   0%|                                                                                               …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5caba44df4794432859b5bac9562a774"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Training:   0%|                                                                                               …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ae5e73dc92754b0091cf64799aabf2f4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Training:   0%|                                                                                               …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "60dc82e0dbf542f4ac15998f961d33b8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_name = 'sp500model_with_riskmodel_period1'\n",
    "num_epochs = 15\n",
    "train_loss_lst = []\n",
    "valid_loss_lst = []\n",
    "pbar = tqdm(range(num_epochs), desc=\"Training\", position=0,dynamic_ncols = True)\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "for e in pbar:\n",
    "    train_loss, train_conloss, train_kl, train_risk = train(factorVAE, train_dataloader, optimizer, batch_size, masked = False)\n",
    "    val_loss, val_conloss, val_kl, val_risk = validate(factorVAE, valid_dataloader, batch_size)\n",
    "    pbar.set_postfix({\"Train Loss\": train_loss, \"Validation Loss\": val_loss})\n",
    "    save_root = os.path.join(save_dir, f'{run_name}_{num_factor}.pt')\n",
    "    torch.save(factorVAE.state_dict(), save_root)\n",
    "    train_loss_lst.append(train_loss)\n",
    "    valid_loss_lst.append(val_loss)\n",
    "    # risk_loss.append([train_risk,val_risk])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def list_to_df(loss_list):\n",
    "    tcl = pd.DataFrame([i[0] for i in loss_list])\n",
    "    vcl = pd.DataFrame([i[1] for i in loss_list])\n",
    "    return tcl, vcl"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "tcl, vcl = pd.DataFrame(train_loss_lst), pd.DataFrame(valid_loss_lst)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "tcl.to_csv('tcl.csv')\n",
    "vcl.to_csv('vcl.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "<AxesSubplot:>"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tcl.plot()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "<AxesSubplot:>"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "vcl.plot()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [
    {
     "data": {
      "text/plain": "0.0"
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}