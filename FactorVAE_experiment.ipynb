{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/jianming/PONet/program/project/FactorVAE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "from accelerate import Accelerator"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "num_epochs = 25\n",
    "lr = 0.0001\n",
    "batch_size = 512\n",
    "batch_gap = 1\n",
    "num_latent = 6 # feature size\n",
    "seq_len = 60\n",
    "num_factor = 16\n",
    "hidden_size = 16\n",
    "hidden_factor = 16\n",
    "seed = 42\n",
    "save_dir = r'/home/jianming/PONet/program/project/FactorVAE/data/best_models'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# train_df = pd.read_pickle(\"/home/jianming/PONet/program/project/FactorVAE/data/train_sp500_QLIB_False_NORM_False_CHAR_False_LEN_20.pkl\")\n",
    "# valid_df = pd.read_pickle(\"/home/jianming/PONet/program/project/FactorVAE/data/valid_sp500_QLIB_False_NORM_False_CHAR_False_LEN_20.pkl\")\n",
    "# test_df = pd.read_pickle(\"/home/jianming/PONet/program/project/FactorVAE/data/test_sp500_QLIB_False_NORM_False_CHAR_False_LEN_20.pkl\")\n",
    "# US_feature_dataset_market_sp500_valid_start2015-01-01_end2016-12-31_label5\n",
    "train_df = pd.read_pickle(\"/home/jianming/PONet/program/project/FactorVAE/data/US_feature_dataset_market_sp500_train_start2008-01-01_end2014-12-31_label1\")\n",
    "valid_df = pd.read_pickle(\"/home/jianming/PONet/program/project/FactorVAE/data/US_feature_dataset_market_sp500_valid_start2015-01-01_end2016-12-31_label1\")\n",
    "test_df = pd.read_pickle(\"/home/jianming/PONet/program/project/FactorVAE/data/US_feature_dataset_market_sp500_test_start2017-01-01_end2020-08-01_label1\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "                        CLOSE59   CLOSE58   CLOSE57   CLOSE56   CLOSE55  \\\ndatetime   instrument                                                     \n2010-01-04 A          -0.801470 -0.792347 -0.805499 -0.865277 -0.789545   \n           AA         -0.939941 -1.010894 -1.017612 -1.105897 -1.004793   \n           AAPL       -0.721489 -0.681588 -0.676562 -0.719535 -0.676018   \n           ABC        -1.130259 -1.073238 -1.041954 -1.094959 -0.954652   \n           ABT        -0.469372 -0.482687 -0.503474 -0.503206 -0.250961   \n...                         ...       ...       ...       ...       ...   \n2017-12-29 XYL        -0.202851 -0.197502 -0.190815 -0.199200 -0.248474   \n           YUM        -0.490197 -0.351213 -0.350005 -0.336936 -0.292932   \n           ZBH         0.177233  0.181779  0.049109  0.112167  0.126088   \n           ZION       -0.277664 -0.246040 -0.347252 -0.288158 -0.372653   \n           ZTS        -0.728868 -0.739231 -0.751547 -0.728709 -0.702852   \n\n                        CLOSE54   CLOSE53   CLOSE52   CLOSE51   CLOSE50  ...  \\\ndatetime   instrument                                                    ...   \n2010-01-04 A          -0.792142 -0.926031 -0.881090 -1.176907 -1.066803  ...   \n           AA         -0.995895 -1.188350 -1.186017 -1.361593 -1.361682  ...   \n           AAPL       -0.718102 -0.838528 -0.770304 -0.387089 -0.119082  ...   \n           ABC        -0.870498 -0.819805 -0.866395 -0.779866 -0.878006  ...   \n           ABT        -0.137176 -0.225172 -0.092749 -0.163453 -0.294764  ...   \n...                         ...       ...       ...       ...       ...  ...   \n2017-12-29 XYL        -0.178195 -0.207453 -0.041043 -0.426306 -0.371775  ...   \n           YUM        -0.326184 -0.357495 -0.363535 -0.374415 -0.402481  ...   \n           ZBH         0.274740  0.200915  0.183904  0.213350  0.271014  ...   \n           ZION       -0.482223 -0.512960 -0.486433 -0.645252 -0.622052  ...   \n           ZTS        -0.632751 -0.574249 -0.548104 -0.545166 -0.588158  ...   \n\n                        VOLUME9   VOLUME8   VOLUME7   VOLUME6   VOLUME5  \\\ndatetime   instrument                                                     \n2010-01-04 A           0.411700  0.298818 -0.209210 -0.420662 -1.603741   \n           AA          0.294136  2.791438  0.451646  0.003445 -0.998537   \n           AAPL        0.458316  0.476913 -0.632760 -0.662556  0.012183   \n           ABC         2.104521  0.139758 -0.390253  0.114148 -1.781350   \n           ABT         1.649119  0.253572 -0.155324 -1.024470 -1.570560   \n...                         ...       ...       ...       ...       ...   \n2017-12-29 XYL         1.498057  0.061790 -0.636312 -1.049967 -0.850350   \n           YUM         3.000000  0.647190 -0.203018 -0.321770  0.491947   \n           ZBH         2.270499  0.882518  3.000000  1.118766  0.122109   \n           ZION        3.000000  2.210243  0.105724  0.725600  0.580410   \n           ZTS         1.355472 -0.076837  0.280258  0.108470 -0.714964   \n\n                        VOLUME4   VOLUME3   VOLUME2   VOLUME1  VOLUME0  \ndatetime   instrument                                                   \n2010-01-04 A          -0.879167  0.303500  1.125745  1.020440      0.0  \n           AA         -0.611950 -1.061957 -0.739595 -1.068296      0.0  \n           AAPL        0.669912 -0.247133 -0.428418 -0.849569      0.0  \n           ABC        -1.233891 -0.973650 -0.862436 -1.182416      0.0  \n           ABT        -0.494855 -0.351178 -0.783187 -1.242245      0.0  \n...                         ...       ...       ...       ...      ...  \n2017-12-29 XYL        -1.539819 -1.857334 -1.269359 -1.614869      0.0  \n           YUM        -0.728033 -1.193908 -0.561972 -1.460446      0.0  \n           ZBH        -0.691201 -1.495628 -0.479991 -0.122525      0.0  \n           ZION       -0.695005 -0.854849 -1.130369 -0.678290      0.0  \n           ZTS        -0.487750 -1.263578 -0.808953 -1.696340      0.0  \n\n[865331 rows x 360 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>CLOSE59</th>\n      <th>CLOSE58</th>\n      <th>CLOSE57</th>\n      <th>CLOSE56</th>\n      <th>CLOSE55</th>\n      <th>CLOSE54</th>\n      <th>CLOSE53</th>\n      <th>CLOSE52</th>\n      <th>CLOSE51</th>\n      <th>CLOSE50</th>\n      <th>...</th>\n      <th>VOLUME9</th>\n      <th>VOLUME8</th>\n      <th>VOLUME7</th>\n      <th>VOLUME6</th>\n      <th>VOLUME5</th>\n      <th>VOLUME4</th>\n      <th>VOLUME3</th>\n      <th>VOLUME2</th>\n      <th>VOLUME1</th>\n      <th>VOLUME0</th>\n    </tr>\n    <tr>\n      <th>datetime</th>\n      <th>instrument</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">2010-01-04</th>\n      <th>A</th>\n      <td>-0.801470</td>\n      <td>-0.792347</td>\n      <td>-0.805499</td>\n      <td>-0.865277</td>\n      <td>-0.789545</td>\n      <td>-0.792142</td>\n      <td>-0.926031</td>\n      <td>-0.881090</td>\n      <td>-1.176907</td>\n      <td>-1.066803</td>\n      <td>...</td>\n      <td>0.411700</td>\n      <td>0.298818</td>\n      <td>-0.209210</td>\n      <td>-0.420662</td>\n      <td>-1.603741</td>\n      <td>-0.879167</td>\n      <td>0.303500</td>\n      <td>1.125745</td>\n      <td>1.020440</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>AA</th>\n      <td>-0.939941</td>\n      <td>-1.010894</td>\n      <td>-1.017612</td>\n      <td>-1.105897</td>\n      <td>-1.004793</td>\n      <td>-0.995895</td>\n      <td>-1.188350</td>\n      <td>-1.186017</td>\n      <td>-1.361593</td>\n      <td>-1.361682</td>\n      <td>...</td>\n      <td>0.294136</td>\n      <td>2.791438</td>\n      <td>0.451646</td>\n      <td>0.003445</td>\n      <td>-0.998537</td>\n      <td>-0.611950</td>\n      <td>-1.061957</td>\n      <td>-0.739595</td>\n      <td>-1.068296</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>AAPL</th>\n      <td>-0.721489</td>\n      <td>-0.681588</td>\n      <td>-0.676562</td>\n      <td>-0.719535</td>\n      <td>-0.676018</td>\n      <td>-0.718102</td>\n      <td>-0.838528</td>\n      <td>-0.770304</td>\n      <td>-0.387089</td>\n      <td>-0.119082</td>\n      <td>...</td>\n      <td>0.458316</td>\n      <td>0.476913</td>\n      <td>-0.632760</td>\n      <td>-0.662556</td>\n      <td>0.012183</td>\n      <td>0.669912</td>\n      <td>-0.247133</td>\n      <td>-0.428418</td>\n      <td>-0.849569</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>ABC</th>\n      <td>-1.130259</td>\n      <td>-1.073238</td>\n      <td>-1.041954</td>\n      <td>-1.094959</td>\n      <td>-0.954652</td>\n      <td>-0.870498</td>\n      <td>-0.819805</td>\n      <td>-0.866395</td>\n      <td>-0.779866</td>\n      <td>-0.878006</td>\n      <td>...</td>\n      <td>2.104521</td>\n      <td>0.139758</td>\n      <td>-0.390253</td>\n      <td>0.114148</td>\n      <td>-1.781350</td>\n      <td>-1.233891</td>\n      <td>-0.973650</td>\n      <td>-0.862436</td>\n      <td>-1.182416</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>ABT</th>\n      <td>-0.469372</td>\n      <td>-0.482687</td>\n      <td>-0.503474</td>\n      <td>-0.503206</td>\n      <td>-0.250961</td>\n      <td>-0.137176</td>\n      <td>-0.225172</td>\n      <td>-0.092749</td>\n      <td>-0.163453</td>\n      <td>-0.294764</td>\n      <td>...</td>\n      <td>1.649119</td>\n      <td>0.253572</td>\n      <td>-0.155324</td>\n      <td>-1.024470</td>\n      <td>-1.570560</td>\n      <td>-0.494855</td>\n      <td>-0.351178</td>\n      <td>-0.783187</td>\n      <td>-1.242245</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">2017-12-29</th>\n      <th>XYL</th>\n      <td>-0.202851</td>\n      <td>-0.197502</td>\n      <td>-0.190815</td>\n      <td>-0.199200</td>\n      <td>-0.248474</td>\n      <td>-0.178195</td>\n      <td>-0.207453</td>\n      <td>-0.041043</td>\n      <td>-0.426306</td>\n      <td>-0.371775</td>\n      <td>...</td>\n      <td>1.498057</td>\n      <td>0.061790</td>\n      <td>-0.636312</td>\n      <td>-1.049967</td>\n      <td>-0.850350</td>\n      <td>-1.539819</td>\n      <td>-1.857334</td>\n      <td>-1.269359</td>\n      <td>-1.614869</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>YUM</th>\n      <td>-0.490197</td>\n      <td>-0.351213</td>\n      <td>-0.350005</td>\n      <td>-0.336936</td>\n      <td>-0.292932</td>\n      <td>-0.326184</td>\n      <td>-0.357495</td>\n      <td>-0.363535</td>\n      <td>-0.374415</td>\n      <td>-0.402481</td>\n      <td>...</td>\n      <td>3.000000</td>\n      <td>0.647190</td>\n      <td>-0.203018</td>\n      <td>-0.321770</td>\n      <td>0.491947</td>\n      <td>-0.728033</td>\n      <td>-1.193908</td>\n      <td>-0.561972</td>\n      <td>-1.460446</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>ZBH</th>\n      <td>0.177233</td>\n      <td>0.181779</td>\n      <td>0.049109</td>\n      <td>0.112167</td>\n      <td>0.126088</td>\n      <td>0.274740</td>\n      <td>0.200915</td>\n      <td>0.183904</td>\n      <td>0.213350</td>\n      <td>0.271014</td>\n      <td>...</td>\n      <td>2.270499</td>\n      <td>0.882518</td>\n      <td>3.000000</td>\n      <td>1.118766</td>\n      <td>0.122109</td>\n      <td>-0.691201</td>\n      <td>-1.495628</td>\n      <td>-0.479991</td>\n      <td>-0.122525</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>ZION</th>\n      <td>-0.277664</td>\n      <td>-0.246040</td>\n      <td>-0.347252</td>\n      <td>-0.288158</td>\n      <td>-0.372653</td>\n      <td>-0.482223</td>\n      <td>-0.512960</td>\n      <td>-0.486433</td>\n      <td>-0.645252</td>\n      <td>-0.622052</td>\n      <td>...</td>\n      <td>3.000000</td>\n      <td>2.210243</td>\n      <td>0.105724</td>\n      <td>0.725600</td>\n      <td>0.580410</td>\n      <td>-0.695005</td>\n      <td>-0.854849</td>\n      <td>-1.130369</td>\n      <td>-0.678290</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>ZTS</th>\n      <td>-0.728868</td>\n      <td>-0.739231</td>\n      <td>-0.751547</td>\n      <td>-0.728709</td>\n      <td>-0.702852</td>\n      <td>-0.632751</td>\n      <td>-0.574249</td>\n      <td>-0.548104</td>\n      <td>-0.545166</td>\n      <td>-0.588158</td>\n      <td>...</td>\n      <td>1.355472</td>\n      <td>-0.076837</td>\n      <td>0.280258</td>\n      <td>0.108470</td>\n      <td>-0.714964</td>\n      <td>-0.487750</td>\n      <td>-1.263578</td>\n      <td>-0.808953</td>\n      <td>-1.696340</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>865331 rows × 360 columns</p>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['feature']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(\n",
    "        self,\n",
    "        df_feature,\n",
    "        df_label,\n",
    "        d_feat = 6,\n",
    "        batch_size=512,\n",
    "        batch_gap = 20,\n",
    "        shuffle=False,\n",
    "        device=None,\n",
    "    ):\n",
    "        assert len(df_feature) == len(df_label)\n",
    "        self.device = device\n",
    "\n",
    "        self.df_feature = df_feature\n",
    "        self.df_label = df_label\n",
    "\n",
    "        self.batch_gap = batch_gap\n",
    "        self.feature = torch.from_numpy(self.df_feature.values)\n",
    "        self.label = torch.from_numpy(self.df_label.values)\n",
    "\n",
    "        self.d_feat = d_feat\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label) // (self.batch_size*self.batch_gap)\n",
    "\n",
    "    def iter_batch(self):\n",
    "        indices = np.arange(len(self.label))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(indices)\n",
    "\n",
    "        for i in range(len(indices))[:: (self.batch_size*self.batch_gap)]:\n",
    "            if len(indices) - i < self.batch_size:\n",
    "                break\n",
    "            yield i, indices[i : i + self.batch_size]\n",
    "\n",
    "    def get_batch(self, i, slc):\n",
    "        outs = self.feature[slc].view(len(slc),self.d_feat, -1), self.label[slc]# No date in iter batch\n",
    "        return outs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_df['feature'].fillna(0),train_df['label'].fillna(0), batch_gap = 1)\n",
    "valid_dataloader = DataLoader(valid_df['feature'].fillna(0),valid_df['label'].fillna(0), batch_gap = 1)\n",
    "test_dataloader = DataLoader(test_df['feature'].fillna(0),test_df['label'].fillna(0), batch_gap = 1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "1690"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, num_latent, hidden_size, num_layers=1):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        self.num_latent = num_latent\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.normalize = nn.LayerNorm(num_latent)\n",
    "        self.linear = nn.Linear(num_latent, num_latent)\n",
    "        self.leakyrelu = nn.LeakyReLU()\n",
    "        self.gru = nn.GRU(num_latent, hidden_size, num_layers, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #! x: (batch_size, seq_length, num_latent)\n",
    "        # Apply linear and LeakyReLU activation\n",
    "        #* layer norm\n",
    "        if x.shape[-1] != self.num_latent:\n",
    "            x = x.permute((0,2,1))\n",
    "        x = self.normalize(x)\n",
    "        out = self.linear(x)\n",
    "        out = self.leakyrelu(out)\n",
    "        # Forward propagate GRU\n",
    "        stock_latent, _ = self.gru(out)\n",
    "        return stock_latent[:,-1,:] #* stock_latent[-1]: (batch_size, hidden_size)\n",
    "\n",
    "class FactorEncoder(nn.Module):\n",
    "    def __init__(self, num_factors, num_portfolio, hidden_size):\n",
    "        super(FactorEncoder, self).__init__()\n",
    "        self.num_factors = num_factors\n",
    "        self.linear = nn.Linear(hidden_size, num_portfolio)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "        self.linear2 = nn.Linear(num_portfolio, num_factors)\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "    def mapping_layer(self, portfolio_return):\n",
    "        #! portfolio_return: (batch_size, 1)\n",
    "        #! mapping layer\n",
    "        # print(portfolio_return.shape)\n",
    "        mean = self.linear2(portfolio_return.squeeze(1))\n",
    "        sigma = self.softplus(mean)\n",
    "        return mean, sigma\n",
    "\n",
    "    def forward(self, stock_latent, returns):\n",
    "        #! stock_latent: (batch_size, hidden_size)\n",
    "        #! returns: (batch_size, 1)\n",
    "        #! make portfolio\n",
    "        weights = self.linear(stock_latent)\n",
    "        weights = self.softmax(weights) # (batch_size, num_portfolio)\n",
    "\n",
    "        # multiply weights and returns\n",
    "        #print(f\"weights shape: {weights.shape}, returns shape: {returns.shape}\") # [300, 20], [300, 1]\n",
    "        # check returns.shape is tuple\n",
    "        if returns.dim() == 1:\n",
    "            returns = returns.unsqueeze(1)\n",
    "        portfolio_return = torch.mm(weights.transpose(1,0), returns) #* portfolio_return: (M, 1)\n",
    "        #print(f\"portfolio_return shape: {portfolio_return.shape}\")\n",
    "\n",
    "        return self.mapping_layer(portfolio_return)\n",
    "\n",
    "class AlphaLayer(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(AlphaLayer, self).__init__()\n",
    "        self.linear1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.leakyrelu = nn.LeakyReLU()\n",
    "        self.mu_layer = nn.Linear(hidden_size, 1)\n",
    "        self.sigma_layer = nn.Linear(hidden_size, 1)\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "    def forward(self, stock_latent):\n",
    "        #* stock latent는 FeatureExtractor에서 나온 것 (batch_size, hidden_size)\n",
    "        stock_latent = self.linear1(stock_latent)\n",
    "        stock_latent = self.leakyrelu(stock_latent)\n",
    "        alpha_mu = self.mu_layer(stock_latent)\n",
    "        alpha_sigma = self.sigma_layer(stock_latent)\n",
    "        return alpha_mu, self.softplus(alpha_sigma)\n",
    "\n",
    "class BetaLayer(nn.Module):\n",
    "    \"\"\"calcuate factor exposure beta(N*K)\"\"\"\n",
    "    def __init__(self, hidden_size, num_factors):\n",
    "        super(BetaLayer, self).__init__()\n",
    "        self.linear1 = nn.Linear(hidden_size, num_factors)\n",
    "\n",
    "    def forward(self, stock_latent):\n",
    "        beta = self.linear1(stock_latent)\n",
    "        return beta\n",
    "\n",
    "class FactorDecoder(nn.Module):\n",
    "    def __init__(self, alpha_layer, beta_layer):\n",
    "        super(FactorDecoder, self).__init__()\n",
    "\n",
    "        self.alpha_layer = alpha_layer\n",
    "        self.beta_layer = beta_layer\n",
    "\n",
    "    def reparameterize(self, mu, sigma):\n",
    "        eps = torch.randn_like(sigma)\n",
    "        return mu + eps * sigma\n",
    "\n",
    "    def return_mu(self,stock_latent, factor_mu, factor_sigma):\n",
    "        alpha_mu, alpha_sigma = self.alpha_layer(stock_latent)\n",
    "        #print(f\"alpha_mu shape: {alpha_mu.shape}, alpha_sigma shape: {alpha_sigma.shape}\")\n",
    "        beta = self.beta_layer(stock_latent)\n",
    "\n",
    "        factor_mu = factor_mu.view(-1, 1)\n",
    "        factor_sigma = factor_sigma.view(-1, 1)\n",
    "\n",
    "        # Replace any zero values in factor_sigma with a small value\n",
    "        factor_sigma[factor_sigma == 0] = 1e-6\n",
    "        #print(f\"factor_mu shape: {factor_mu.shape}, factor_sigma shape: {factor_sigma.shape}\")\n",
    "        #print(f\"beta shape: {beta.shape}\")\n",
    "        mu = alpha_mu + torch.matmul(beta, factor_mu)\n",
    "        return mu\n",
    "\n",
    "    def forward(self, stock_latent, factor_mu, factor_sigma):\n",
    "        #! warning: alpha_mu, alpha_sigma -> (N), (N)\n",
    "        alpha_mu, alpha_sigma = self.alpha_layer(stock_latent)\n",
    "        #print(f\"alpha_mu shape: {alpha_mu.shape}, alpha_sigma shape: {alpha_sigma.shape}\")\n",
    "        beta = self.beta_layer(stock_latent)\n",
    "\n",
    "        factor_mu = factor_mu.view(-1, 1)\n",
    "        factor_sigma = factor_sigma.view(-1, 1)\n",
    "\n",
    "        # Replace any zero values in factor_sigma with a small value\n",
    "        factor_sigma[factor_sigma == 0] = 1e-6\n",
    "        #print(f\"factor_mu shape: {factor_mu.shape}, factor_sigma shape: {factor_sigma.shape}\")\n",
    "        #print(f\"beta shape: {beta.shape}\")\n",
    "        mu = alpha_mu + torch.matmul(beta, factor_mu)\n",
    "        sigma = torch.sqrt(alpha_sigma**2 + torch.matmul(beta**2, factor_sigma**2) + 1e-6)\n",
    "        return self.reparameterize(mu, sigma)\n",
    "\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "\n",
    "        self.query = nn.Parameter(torch.randn(hidden_size))\n",
    "        self.key_layer = nn.Linear(hidden_size, hidden_size)\n",
    "        self.value_layer = nn.Linear(hidden_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, stock_latent):\n",
    "        #* calculate attention weights\n",
    "\n",
    "        self.key = self.key_layer(stock_latent)\n",
    "        self.value = self.value_layer(stock_latent)\n",
    "\n",
    "        attention_weights = torch.matmul(self.query, self.key.transpose(1,0)) # (N)\n",
    "        #* scaling\n",
    "        attention_weights = attention_weights / torch.sqrt(torch.tensor(self.key.shape[0])+ 1e-6)\n",
    "        # print(f\"attention_weights shape: {attention_weights.shape}\")\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        attention_weights = F.relu(attention_weights) # max(0, x)\n",
    "        attention_weights = F.softmax(attention_weights, dim=0) # (N)\n",
    "\n",
    "        #! calculate context vector\n",
    "        if torch.isnan(attention_weights).any() or torch.isinf(attention_weights).any():\n",
    "            return torch.zeros_like(self.value[0])\n",
    "        else:\n",
    "            context_vector = torch.matmul(attention_weights, self.value) # (H)\n",
    "            return context_vector\n",
    "\n",
    "class FactorPredictor(nn.Module):\n",
    "    def __init__(self, batch_size, hidden_size, num_factor):\n",
    "        super(FactorPredictor, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        self.num_factor = num_factor\n",
    "        self.attention_layers = nn.ModuleList([AttentionLayer(self.hidden_size) for _ in range(num_factor)])\n",
    "\n",
    "        self.linear = nn.Linear(hidden_size, hidden_size)\n",
    "        self.leakyrelu = nn.LeakyReLU()\n",
    "        self.mu_layer = nn.Linear(hidden_size, 1)\n",
    "        self.sigma_layer = nn.Linear(hidden_size, 1)\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "    def forward(self, stock_latent):\n",
    "        for i in range(self.num_factor):\n",
    "            attention_layer = self.attention_layers[i](stock_latent)\n",
    "            if i == 0:\n",
    "                h_multi = attention_layer\n",
    "            else:\n",
    "                h_multi = torch.cat((h_multi, attention_layer), dim=0)\n",
    "        h_multi = h_multi.view(self.num_factor, -1)\n",
    "\n",
    "        # print(\"h_multi:\", h_multi.shape)\n",
    "        h_multi = self.linear(h_multi)\n",
    "        h_multi = self.leakyrelu(h_multi)\n",
    "        pred_mu = self.mu_layer(h_multi)\n",
    "        pred_sigma = self.sigma_layer(h_multi)\n",
    "        pred_sigma = self.softplus(pred_sigma)\n",
    "        pred_mu = pred_mu.view(-1)\n",
    "        pred_sigma = pred_sigma.view(-1)\n",
    "        return pred_mu, pred_sigma\n",
    "\n",
    "class FactorVAE(nn.Module):\n",
    "    def __init__(self, feature_extractor, factor_encoder, factor_decoder, factor_predictor, risk_extrator, hidden_factor):\n",
    "        super(FactorVAE, self).__init__()\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.factor_encoder = factor_encoder\n",
    "        self.factor_decoder = factor_decoder\n",
    "        self.factor_predictor = factor_predictor\n",
    "        self.risk_extrator = risk_extrator\n",
    "        self.hidden_factor = hidden_factor\n",
    "        self.map = nn.Linear(32,hidden_factor)\n",
    "\n",
    "    @staticmethod\n",
    "    def KL_Divergence(mu1, sigma1, mu2, sigma2):\n",
    "        #! mu1, mu2: (batch_size, 1)\n",
    "        #! sigma1, sigma2: (batch_size, 1)\n",
    "        #! output: (batch_size, 1)\n",
    "        kl_div = (torch.log(sigma2/ sigma1) + (sigma1**2 + (mu1 - mu2)**2) / (2 * sigma2**2) - 0.5).sum()\n",
    "        return kl_div\n",
    "\n",
    "    def forward(self, x, returns):\n",
    "        #! x: (batch_size, seq_length, num_latent)\n",
    "        #! returns: (batch_size, 1)\n",
    "\n",
    "        stock_latent = self.feature_extractor(x)\n",
    "        risk_latent = self.risk_extrator(x)\n",
    "        stock_latent = torch.cat([stock_latent,risk_latent],dim=1)\n",
    "        stock_latent = self.map(stock_latent)\n",
    "        # corr = torch.corrcoef(stock_latent.transpose(1,0))\n",
    "        # U,_,_ = torch.pca_lowrank(corr, q=self.hidden_factor, center=True, niter=2)\n",
    "        # stock_latent = torch.mm(stock_latent,U)/torch.sqrt(torch.Tensor(len(stock_latent)))\n",
    "\n",
    "        factor_mu, factor_sigma = self.factor_encoder(stock_latent, returns)\n",
    "        reconstruction = self.factor_decoder(stock_latent, factor_mu, factor_sigma)\n",
    "        pred_mu, pred_sigma = self.factor_predictor(stock_latent)\n",
    "\n",
    "        # print(f\"pred_mu: {pred_mu.shape}, pred_sigma: {pred_sigma.shape}\")\n",
    "        # Define VAE loss function with reconstruction loss and KL divergence\n",
    "        #* Some adjustment\n",
    "        #* stock_adj: number of stocks that have no return data\n",
    "        stock_adj = 0\n",
    "        for i in range(len(returns)-1,-1,-1):\n",
    "            if returns[i] == 0:\n",
    "                stock_adj += 1\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        if stock_adj > 0:\n",
    "            reconstruction_loss = F.mse_loss(reconstruction[:-stock_adj], returns[:-stock_adj])\n",
    "        else:\n",
    "            reconstruction_loss = F.mse_loss(reconstruction, returns)\n",
    "\n",
    "        # Calculate KL divergence between two Gaussian distributions\n",
    "        if torch.any(pred_sigma == 0):\n",
    "            pred_sigma[pred_sigma == 0] = 1e-6\n",
    "        kl_divergence = self.KL_Divergence(factor_mu, factor_sigma, pred_mu, pred_sigma)\n",
    "        # regularization_loss = 0\n",
    "        # prediction_loss = F.mse_loss(self.prediction(x),returns)\n",
    "        vae_loss = 1.5*reconstruction_loss + 0.5*kl_divergence\n",
    "        # print(\"loss: \", vae_loss)\n",
    "        return vae_loss, reconstruction, factor_mu, factor_sigma, pred_mu, pred_sigma #! reconstruction, factor_mu, factor_sigma\n",
    "\n",
    "    # 학습 이후 사용\n",
    "    def prediction(self, x):\n",
    "        stock_latent = self.feature_extractor(x)\n",
    "        pred_mu, pred_sigma = self.factor_predictor(stock_latent)\n",
    "        y_pred = self.factor_decoder.return_mu(stock_latent, pred_mu, pred_sigma)\n",
    "        return y_pred\n",
    "\n",
    "    def latent_factor(self, x):\n",
    "        stock_latent = self.feature_extractor(x)\n",
    "        pred_mu, pred_sigma = self.factor_predictor(stock_latent)\n",
    "        return (pred_mu, pred_sigma)\n",
    "\n",
    "class GATModel(nn.Module):\n",
    "    def __init__(self, d_feat=158, hidden_size=64, num_layers=2, dropout=0.0, base_model=\"GRU\"):\n",
    "        super().__init__()\n",
    "\n",
    "        if base_model == \"GRU\":\n",
    "            self.rnn = nn.GRU(\n",
    "                input_size=d_feat,\n",
    "                hidden_size=hidden_size,\n",
    "                num_layers=num_layers,\n",
    "                batch_first=True,\n",
    "                dropout=dropout,\n",
    "            )\n",
    "        elif base_model == \"LSTM\":\n",
    "            self.rnn = nn.LSTM(\n",
    "                input_size=d_feat,\n",
    "                hidden_size=hidden_size,\n",
    "                num_layers=num_layers,\n",
    "                batch_first=True,\n",
    "                dropout=dropout,\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"unknown base model name `%s`\" % base_model)\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.d_feat = d_feat\n",
    "        self.transformation = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.a = nn.Parameter(torch.randn(self.hidden_size * 2, 1))\n",
    "        self.a.requires_grad = True\n",
    "        self.fc = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.fc_out = nn.Linear(hidden_size, hidden_size)\n",
    "        self.leaky_relu = nn.LeakyReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def cal_attention(self, x, y):\n",
    "        x = self.transformation(x)\n",
    "        y = self.transformation(y)\n",
    "\n",
    "        sample_num = x.shape[0]\n",
    "        dim = x.shape[1]\n",
    "        e_x = x.expand(sample_num, sample_num, dim)\n",
    "        e_y = torch.transpose(e_x, 0, 1)\n",
    "        attention_in = torch.cat((e_x, e_y), 2).view(-1, dim * 2)\n",
    "        self.a_t = torch.t(self.a)\n",
    "        attention_out = self.a_t.mm(torch.t(attention_in)).view(sample_num, sample_num)\n",
    "        attention_out = self.leaky_relu(attention_out)\n",
    "        att_weight = self.softmax(attention_out)\n",
    "        return att_weight\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [N, F*T]\n",
    "        x = x.reshape(len(x), self.d_feat, -1)  # [N, F, T]\n",
    "        x = x.permute(0, 2, 1)  # [N, T, F]\n",
    "        out, _ = self.rnn(x)\n",
    "        hidden = out[:, -1, :]\n",
    "        att_weight = self.cal_attention(hidden, hidden)\n",
    "        hidden = att_weight.mm(hidden) + hidden\n",
    "        hidden = self.fc(hidden)\n",
    "        hidden = self.leaky_relu(hidden)\n",
    "        return self.fc_out(hidden).squeeze() # [N, hidden size]\n",
    "\n",
    "class RiskFeatureExtractor(nn.Module):\n",
    "    \"\"\"supervise variance-covariance matrix reconstuction\"\"\"\n",
    "    def __init__(self, num_latent, hidden_size,num_layers=2,dropout=0.2,base_model = 'GRU'):\n",
    "        super(RiskFeatureExtractor, self).__init__()\n",
    "        self.num_latent = num_latent\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.normalize = nn.LayerNorm(self.num_latent)\n",
    "        self.linear = nn.Linear(self.num_latent, self.num_latent)\n",
    "        self.leakyrelu = nn.LeakyReLU()\n",
    "        self.GAT_model = GATModel(\n",
    "            d_feat=self.num_latent,\n",
    "            hidden_size=self.hidden_size,\n",
    "            num_layers=num_layers ,\n",
    "            dropout=dropout,\n",
    "            base_model=base_model,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.shape[-1] != self.num_latent:\n",
    "            x = x.permute((0,2,1))\n",
    "        x = self.normalize(x)\n",
    "        out = self.linear(x)\n",
    "        out = self.leakyrelu(out)\n",
    "        risk_latent = self.GAT_model(out)\n",
    "        return risk_latent #* stock_latent[-1]: (batch_size, hidden_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "set_seed(seed)\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "# create model\n",
    "feature_extractor = FeatureExtractor(num_latent=num_latent, hidden_size=hidden_size)\n",
    "risk_extrator = RiskFeatureExtractor(num_latent=num_latent, hidden_size = hidden_size)\n",
    "factor_encoder = FactorEncoder(num_factors=num_factor, num_portfolio=num_factor, hidden_size=hidden_factor)\n",
    "alpha_layer = AlphaLayer(hidden_factor)\n",
    "beta_layer = BetaLayer(hidden_factor, num_factor)\n",
    "factor_decoder = FactorDecoder(alpha_layer, beta_layer)\n",
    "factor_predictor = FactorPredictor(batch_size, hidden_factor, num_factor)\n",
    "factorVAE = FactorVAE(feature_extractor, factor_encoder, factor_decoder, factor_predictor, risk_extrator, hidden_factor)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "def train(factor_model, dataloader, optimizer, batch_size, masked = False, benchmark = False):\n",
    "    factor_model.to(device)\n",
    "    factor_model.train()\n",
    "    total_loss = 0\n",
    "    reconstruction_loss = 0\n",
    "    kl_loss = 0\n",
    "    risk_loss = 0\n",
    "    ic = 0\n",
    "    mask_id = torch.randint(0, 500, (30,))\n",
    "    pbar = tqdm(range(len(dataloader)), desc=\"Training\", position=0,dynamic_ncols = True)\n",
    "    for (i, slc),_ in zip(dataloader.iter_batch(),pbar):\n",
    "        char, returns = dataloader.get_batch(i, slc)\n",
    "        if char.size(0)!=batch_size:\n",
    "            continue\n",
    "        inputs = char\n",
    "        labels = returns\n",
    "        inputs = inputs.float().to(device)\n",
    "        labels = labels.float().to(device)\n",
    "\n",
    "        if masked:\n",
    "            inputs[mask_id,:,:] = 0\n",
    "\n",
    "        if torch.isnan(inputs).any() or torch.isnan(labels).any():\n",
    "            continue\n",
    "        optimizer.zero_grad()\n",
    "        if benchmark:\n",
    "            pred = factor_model(inputs.view(batch_size,-1))\n",
    "            reconstruction, factor_mu, factor_sigma, pred_mu, pred_sigma = torch.nan,torch.nan,torch.nan,torch.nan, torch.nan\n",
    "            loss = F.mse_loss(pred.unsqueeze(1),labels)\n",
    "        else:\n",
    "            loss, reconstruction, factor_mu, factor_sigma, pred_mu, pred_sigma = factor_model(inputs, labels)\n",
    "            pred = factor_model.prediction(inputs)\n",
    "        if loss>10:\n",
    "            continue\n",
    "        total_loss += loss.item() * inputs.size(0)\n",
    "        # loss.backward()\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "        nn.utils.clip_grad_norm_(factor_model.parameters(), 1.0)\n",
    "        try:\n",
    "            reconstruction_loss += reconstruction.squeeze().mean().item()\n",
    "            kl_loss += loss.item() + reconstruction.squeeze().mean().item()\n",
    "        except:\n",
    "            reconstruction_loss = 0\n",
    "            kl_loss = 0\n",
    "        # risk_loss += risk.squeeze().mean().item()\n",
    "        # pred = factor_model.prediction(inputs)\n",
    "        ic += np.corrcoef(pred.squeeze().detach().cpu().numpy().T,labels.squeeze().detach().cpu().numpy().T)[1,0]\n",
    "        pbar.set_postfix({\"Train Loss\": loss.item()})\n",
    "\n",
    "        # print(loss)\n",
    "    ic = ic / len(dataloader)\n",
    "    print('ic',ic)\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    reconstruction_loss = reconstruction_loss/len(dataloader)\n",
    "    kl_loss = kl_loss / len(dataloader)\n",
    "    return avg_loss, reconstruction_loss, kl_loss, risk_loss\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(factor_model, dataloader, batch_size, benchmark = False):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    factor_model.to(device)\n",
    "    factor_model.eval()\n",
    "    total_loss = 0\n",
    "    reconstruction_loss = 0\n",
    "    ic = 0\n",
    "    kl_loss = 0\n",
    "    risk_loss = 0\n",
    "    for i, slc in dataloader.iter_batch():\n",
    "        char, returns = dataloader.get_batch(i, slc)\n",
    "        if char.size(0)!=batch_size:\n",
    "            continue\n",
    "        inputs = char\n",
    "        labels = returns[:,-1].reshape(-1,1)\n",
    "        inputs = inputs.float().to(device)\n",
    "        labels = labels.float().to(device)\n",
    "        if benchmark:\n",
    "            pred = factor_model(inputs.view(batch_size,-1))\n",
    "            reconstruction, factor_mu, factor_sigma, pred_mu, pred_sigma = torch.nan,torch.nan,torch.nan,torch.nan, torch.nan\n",
    "            loss = F.mse_loss(pred.unsqueeze(1),labels)\n",
    "        else:\n",
    "            loss, reconstruction, factor_mu, factor_sigma, pred_mu, pred_sigma = factor_model(inputs, labels)\n",
    "            pred = factor_model.prediction(inputs)\n",
    "\n",
    "        total_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        try:\n",
    "            reconstruction_loss += reconstruction.squeeze().mean().item()\n",
    "            kl_loss += loss.item() + reconstruction.squeeze().mean().item()\n",
    "        except:\n",
    "            reconstruction_loss = 0\n",
    "            kl_loss = 0\n",
    "\n",
    "        # reconstruction_loss += reconstruction.squeeze().mean().item()\n",
    "        # # risk_loss += risk.squeeze().mean().item()\n",
    "        # kl_loss += loss.item() + reconstruction.squeeze().mean().item()\n",
    "        ic += np.corrcoef(pred.squeeze().detach().cpu().numpy().T,labels.squeeze().detach().cpu().numpy().T)[1,0]\n",
    "    ic = ic / len(dataloader)\n",
    "    print('Test ic',ic)\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    reconstruction_loss = reconstruction_loss/len(dataloader)\n",
    "    kl_loss = kl_loss / len(dataloader)\n",
    "    return avg_loss, reconstruction_loss, kl_loss, risk_loss\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(factor_model, dataloader, seq_len):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    factor_model.to(device)\n",
    "    factor_model.eval()\n",
    "    total_loss = 0\n",
    "    with tqdm(total=len(dataloader)-seq_len+1) as pbar:\n",
    "        for char, returns,idx in dataloader:\n",
    "            if char.shape[1] != seq_len:\n",
    "                continue\n",
    "            inputs = char.to(device)\n",
    "            labels = returns.to(device)\n",
    "            inputs = inputs.float()\n",
    "            labels = labels.float()\n",
    "\n",
    "            loss, reconstruction, factor_mu, factor_sigma, pred_mu, pred_sigma = factor_model(inputs, labels)\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "            pbar.update(1)\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    return avg_loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, d_feat, hidden_size=128, num_layers=3, dropout=0.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mlp = nn.Sequential()\n",
    "        for i in range(num_layers):\n",
    "            if i > 0:\n",
    "                self.mlp.add_module(\"drop_%d\" % i, nn.Dropout(dropout))\n",
    "            self.mlp.add_module(\"fc_%d\" % i, nn.Linear(d_feat if i == 0 else hidden_size, hidden_size))\n",
    "            self.mlp.add_module(\"bd_%d\" % i, nn.BatchNorm1d(hidden_size))\n",
    "            self.mlp.add_module(\"relu_%d\" % i, nn.ReLU())\n",
    "\n",
    "        self.mlp.add_module(\"fc_out\", nn.Linear(hidden_size, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x).squeeze()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "model = MLP(d_feat = 360).cuda()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "optimizer = torch.optim.Adam(factorVAE.parameters(), lr=0.0001)\n",
    "accelerator = Accelerator()\n",
    "# model_name = '/home/jianming/PONet/program/project/FactorVAE/data/best_models/sp500model_with_riskmodel_period20_64.pt'\n",
    "# factorVAE.load_state_dict(torch.load(model_name))\n",
    "train_dataloader, factorVAE, optimizer = accelerator.prepare(train_dataloader, factorVAE, optimizer)\n",
    "# optimizer = torch.optim.SGD(factorVAE.parameters(), lr=0.0001, momentum=0.9)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "data": {
      "text/plain": "Training:   0%|                                                                                               …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3b9cc9806b414684a8cfd729883438b9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Training:   0%|                                                                                               …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "77046e3165b245aaad74dde0049949e3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ic -0.006179741606775049\n",
      "Test ic -0.020299833745888805\n",
      "Test ic -0.007986389409027057\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training:   0%|                                                                                               …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "94acbaa2d7d34969902a531c71d7ac28"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ic 0.0029211510646903215\n",
      "Test ic -0.01680606725907227\n",
      "Test ic -0.0034127579876208273\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training:   0%|                                                                                               …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a902655e139444f3a7ba2703483c4ccd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_name = 'sp500model_with_riskmodel_periodm302'\n",
    "num_epochs = 25\n",
    "train_loss_lst = []\n",
    "valid_loss_lst = []\n",
    "pbar = tqdm(range(num_epochs), desc=\"Training\", position=0,dynamic_ncols = True)\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "for e in pbar:\n",
    "    train_loss, train_conloss, train_kl, train_risk = train(factorVAE, train_dataloader, optimizer, batch_size, masked = False, benchmark= False)\n",
    "    val_loss, val_conloss, val_kl, val_risk = validate(factorVAE, valid_dataloader, batch_size, benchmark = False)\n",
    "    val_loss, val_conloss, val_kl, val_risk = validate(factorVAE, test_dataloader, batch_size, benchmark = False)\n",
    "    pbar.set_postfix({\"Train Loss\": train_loss, \"Validation Loss\": val_loss})\n",
    "    save_root = os.path.join(save_dir, f'{run_name}_{num_factor}.pt')\n",
    "    torch.save(factorVAE.state_dict(), save_root)\n",
    "    train_loss_lst.append(train_loss)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def list_to_df(loss_list):\n",
    "    tcl = pd.DataFrame([i[0] for i in loss_list])\n",
    "    vcl = pd.DataFrame([i[1] for i in loss_list])\n",
    "    return tcl, vcl"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "tcl, vcl = pd.DataFrame(train_loss_lst), pd.DataFrame(valid_loss_lst)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "tcl.to_csv('tcl.csv')\n",
    "vcl.to_csv('vcl.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "no numeric data to plot",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_5616/3762543817.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mtcl\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mplot\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/anaconda3/envs/env_qlib3.7/lib/python3.7/site-packages/pandas/plotting/_core.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    970\u001B[0m                     \u001B[0mdata\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcolumns\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlabel_name\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    971\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 972\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mplot_backend\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mplot\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkind\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mkind\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    973\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    974\u001B[0m     \u001B[0m__call__\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__doc__\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m__doc__\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/env_qlib3.7/lib/python3.7/site-packages/pandas/plotting/_matplotlib/__init__.py\u001B[0m in \u001B[0;36mplot\u001B[0;34m(data, kind, **kwargs)\u001B[0m\n\u001B[1;32m     69\u001B[0m             \u001B[0mkwargs\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"ax\"\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mgetattr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0max\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"left_ax\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0max\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     70\u001B[0m     \u001B[0mplot_obj\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mPLOT_CLASSES\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mkind\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 71\u001B[0;31m     \u001B[0mplot_obj\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgenerate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     72\u001B[0m     \u001B[0mplot_obj\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdraw\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     73\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mplot_obj\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mresult\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/env_qlib3.7/lib/python3.7/site-packages/pandas/plotting/_matplotlib/core.py\u001B[0m in \u001B[0;36mgenerate\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    284\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mgenerate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    285\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_args_adjust\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 286\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_compute_plot_data\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    287\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_setup_subplots\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    288\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_make_plot\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/env_qlib3.7/lib/python3.7/site-packages/pandas/plotting/_matplotlib/core.py\u001B[0m in \u001B[0;36m_compute_plot_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    451\u001B[0m         \u001B[0;31m# no non-numeric frames or series allowed\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    452\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mis_empty\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 453\u001B[0;31m             \u001B[0;32mraise\u001B[0m \u001B[0mTypeError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"no numeric data to plot\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    454\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    455\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnumeric_data\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mapply\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_convert_to_ndarray\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mTypeError\u001B[0m: no numeric data to plot"
     ]
    }
   ],
   "source": [
    "tcl.plot()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vcl.plot()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [
    {
     "data": {
      "text/plain": "0.0"
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ic -0.01606022309089574\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_conloss, val_kl, val_risk = validate(model, test_dataloader, batch_size, benchmark = True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}